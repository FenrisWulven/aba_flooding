{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is our 'product'\n",
    "- What issues are we attempting to solve/assist? Who are we helping? Why are we doing this?\n",
    "- Data description (What data do we need to be able to build our model?)\n",
    "- Overview of our upcoming sections of code. What is our process?\n",
    "  - API\n",
    "  - Building the predictions (Time series -> Survival Analysis)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bokeh.models import GeoJSONDataSource, LinearColorMapper\n",
    "from bokeh.palettes import Viridis256\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pyproj import Transformer\n",
    "from shapely.ops import transform\n",
    "import fiona\n",
    "\n",
    "# WIll take the a model and geodata and apply the survival function on the data so it is in geojson format for the map\n",
    "\n",
    "def get_data_dir():\n",
    "    \"\"\"Return path to the raw data directory.\"\"\"\n",
    "    current_file = Path(__file__)\n",
    "    project_root = current_file.parent.parent.parent\n",
    "    data_dir = project_root / \"data\" / \"raw\"\n",
    "    return data_dir\n",
    "\n",
    "def load_geojson(file_name):\n",
    "    \"\"\"Load a GeoJSON file from the data directory.\"\"\"\n",
    "    data_dir = get_data_dir()\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    print(f\"Loading GeoJSON from {file_path}\")\n",
    "    try:\n",
    "        # First try standard geopandas approach\n",
    "        gdf = gpd.read_file(file_path)\n",
    "        print(f\"Loaded GeoJSON from {file_name} using standard GeoPandas\")\n",
    "        return gdf\n",
    "    except AttributeError as e:\n",
    "        if \"module 'pyogrio' has no attribute\" in str(e):\n",
    "            print(f\"ERROR loading '{file_name}' due to pyogrio error: {e}\")\n",
    "            try:\n",
    "                # Try using fiona directly\n",
    "                import fiona\n",
    "                with fiona.open(file_path, 'r') as src:\n",
    "                    crs = src.crs\n",
    "                    features = list(src)\n",
    "                \n",
    "                # Convert to GeoDataFrame\n",
    "                import shapely.geometry\n",
    "                geoms = [shapely.geometry.shape(feature['geometry']) for feature in features]\n",
    "                properties = [feature['properties'] for feature in features]\n",
    "                \n",
    "                # Create a GeoDataFrame\n",
    "                gdf = gpd.GeoDataFrame(properties, geometry=geoms, crs=crs)\n",
    "                print(f\"Loaded GeoJSON from {file_name} using fiona engine\")\n",
    "                return gdf\n",
    "            except Exception as fiona_error:\n",
    "                print(f\"ERROR Fiona method failed also: {fiona_error}\")\n",
    "                try:\n",
    "                    # Last resort: manually parse JSON\n",
    "                    import json\n",
    "                    from shapely.geometry import shape\n",
    "                    \n",
    "                    with open(file_path, 'r') as f:\n",
    "                        geojson_dict = json.load(f)\n",
    "                    \n",
    "                    features = geojson_dict.get('features', [])\n",
    "                    geoms = [shape(feature['geometry']) for feature in features]\n",
    "                    properties = [feature['properties'] for feature in features]\n",
    "                    \n",
    "                    gdf = gpd.GeoDataFrame(properties, geometry=geoms)\n",
    "                    if 'crs' in geojson_dict:\n",
    "                        gdf.crs = geojson_dict['crs']\n",
    "                    \n",
    "                    print(f\"Loaded GeoJSON from {file_name} using manual JSON parsing\")\n",
    "                    return gdf\n",
    "                except Exception as json_error:\n",
    "                    print(f\"ERROR Manual JSON parsing failed: {json_error}\")\n",
    "                    print(f\"ERROR: Could not load {file_name}\")\n",
    "                    raise json_error\n",
    "        else:\n",
    "            print(f\"ERROR Could not load GeoJSON {file_name}: {e}\")\n",
    "            raise e\n",
    "    except Exception as general_error:\n",
    "        print(f\"ERROR Could not load GeoJSON {file_name}: {general_error}\")\n",
    "        try:\n",
    "            # Try alternate method with fiona as a general fallback\n",
    "            import fiona\n",
    "            with fiona.open(file_path, 'r') as src:\n",
    "                crs = src.crs\n",
    "                features = list(src)\n",
    "            \n",
    "            # Convert to GeoDataFrame\n",
    "            import shapely.geometry\n",
    "            geoms = [shapely.geometry.shape(feature['geometry']) for feature in features]\n",
    "            properties = [feature['properties'] for feature in features]\n",
    "            \n",
    "            # Create a GeoDataFrame\n",
    "            gdf = gpd.GeoDataFrame(properties, geometry=geoms, crs=crs)\n",
    "            print(f\"Loaded GeoJSON from {file_name} using fallback method\")\n",
    "            return gdf\n",
    "        except Exception as e2:\n",
    "            print(f\"ERROR Alternative loading also failed: {e2}\")\n",
    "            print(f\"ERROR: Could not load {file_name}\")\n",
    "            raise general_error\n",
    "\n",
    "def load_gpkg(file_name, layer=None):\n",
    "    \"\"\"Load a GeoPackage file from the data directory, with optional layer name.\"\"\"\n",
    "    data_dir = get_data_dir()\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    \n",
    "    if layer:\n",
    "        return gpd.read_file(file_path, layer=layer)\n",
    "    else:\n",
    "        # Try to get available layers first\n",
    "        try:\n",
    "            layers = fiona.listlayers(file_path)\n",
    "            if len(layers) > 0:\n",
    "                print(f\"Available layers in {file_name}: {layers}\")\n",
    "                return gpd.read_file(file_path, layer=layers[0])\n",
    "            else:\n",
    "                return gpd.read_file(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting layers: {e}\")\n",
    "            return gpd.read_file(file_path)\n",
    "\n",
    "def load_terrain_data(file_name):\n",
    "    \"\"\"Load terrain data from GeoJSON or GPKG file.\"\"\"\n",
    "    if file_name.endswith('.geojson'):\n",
    "        return load_geojson(file_name)\n",
    "    elif file_name.endswith('.gpkg'):\n",
    "        return load_gpkg(file_name)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format for {file_name}\")\n",
    "\n",
    "def wgs84_to_web_mercator(df):\n",
    "    \"\"\"Convert GeoDataFrame from WGS84 to Web Mercator projection.\"\"\"\n",
    "    transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:3857\", always_xy=True)\n",
    "    \n",
    "    # Create new geometry column with transformed coordinates\n",
    "    df = df.copy()\n",
    "    df['geometry'] = df['geometry'].apply(\n",
    "        lambda geom: transform(lambda x, y: transformer.transform(x, y), geom)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def convert_to_serializable(obj):\n",
    "    \"\"\"Convert non-serializable objects to JSON serializable types.\"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from datetime import datetime, date\n",
    "    \n",
    "    if isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float64, np.float32)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.ndarray,)):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (pd.Timestamp, datetime, date)):\n",
    "        return obj.isoformat()\n",
    "    elif hasattr(obj, 'to_dict'):\n",
    "        return obj.to_dict()\n",
    "    else:\n",
    "        return str(obj)\n",
    "\n",
    "def gdf_to_geojson(gdf):\n",
    "    \"\"\"\n",
    "    Convert a GeoDataFrame to GeoJSON format.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : geopandas.GeoDataFrame\n",
    "        GeoDataFrame to convert\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        GeoJSON string\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Preprocess the dataframe to handle problematic columns\n",
    "    df_copy = gdf.copy()\n",
    "    \n",
    "    # Convert all timestamp columns to strings\n",
    "    for col in df_copy.columns:\n",
    "        if col != 'geometry':\n",
    "            # Check if column has timestamp data\n",
    "            if pd.api.types.is_datetime64_any_dtype(df_copy[col]):\n",
    "                df_copy[col] = df_copy[col].astype(str)\n",
    "            # Convert any numpy data type columns to native Python types\n",
    "            elif pd.api.types.is_numeric_dtype(df_copy[col]):\n",
    "                df_copy[col] = df_copy[col].apply(\n",
    "                    lambda x: float(x) if pd.api.types.is_float_dtype(type(x)) else \n",
    "                    int(x) if pd.api.types.is_integer_dtype(type(x)) else x\n",
    "                )\n",
    "    \n",
    "    # Use a custom serialization approach\n",
    "    try:\n",
    "        class CustomEncoder(json.JSONEncoder):\n",
    "            def default(self, obj):\n",
    "                if isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "                    return int(obj)\n",
    "                elif isinstance(obj, (np.floating, np.float64, np.float32)):\n",
    "                    return float(obj)\n",
    "                elif isinstance(obj, np.ndarray):\n",
    "                    return obj.tolist()\n",
    "                elif isinstance(obj, (pd.Timestamp, datetime)):\n",
    "                    return obj.isoformat()\n",
    "                elif hasattr(obj, 'to_dict'):\n",
    "                    return obj.to_dict()\n",
    "                return json.JSONEncoder.default(self, obj)\n",
    "        \n",
    "        # First convert to GeoJSON dict\n",
    "        geo_dict = json.loads(df_copy.to_json())\n",
    "        \n",
    "        # Then serialize with custom encoder\n",
    "        return json.dumps(geo_dict, cls=CustomEncoder)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in GeoJSON serialization: {e}\")\n",
    "        \n",
    "        # Fallback approach - manually build GeoJSON\n",
    "        features = []\n",
    "        for idx, row in df_copy.iterrows():\n",
    "            try:\n",
    "                properties = {}\n",
    "                for col in df_copy.columns:\n",
    "                    if col != 'geometry':\n",
    "                        val = row[col]\n",
    "                        properties[col] = convert_to_serializable(val)\n",
    "                \n",
    "                geometry = row['geometry'].__geo_interface__\n",
    "                features.append({\n",
    "                    \"type\": \"Feature\",\n",
    "                    \"properties\": properties,\n",
    "                    \"geometry\": geometry\n",
    "                })\n",
    "            except Exception as feat_e:\n",
    "                print(f\"Error processing feature {idx}: {feat_e}\")\n",
    "        \n",
    "        geojson = {\n",
    "            \"type\": \"FeatureCollection\",\n",
    "            \"features\": features\n",
    "        }\n",
    "        \n",
    "        return json.dumps(geojson)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing an API to retrieve percipitation data for all of Denmark ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the neccesary perticipation data, we utilized DMI's (Danmarks Meteorologiske Institut) Meteorological observation API (https://dmiapi.govcloud.dk). This API granted us access to 86 weather observation stations, located all accross Denmark. This was an extensive data extraction process, which required us to divide the extraction into mulitple json files, and finally collect the data into a single parquet file. The API also provided us with the longitude and lattitude of each station, which was utilized in the subsequent mapping of the stations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#=================================================================================================#\n",
    "# This script retrieves weather data from the DMI API for all stations in Denmark.\n",
    "# It saves the data to JSON files and creates a map visualization using Folium.\n",
    "# It handles pagination, retries, and error handling for API requests.\n",
    "# It also includes logging for better tracking of the process.\n",
    "#=================================================================================================#\n",
    "\n",
    "import requests, os, json, folium, logging, time, random, gc, time, logging\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"dmi_api.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Define your API key\n",
    "api_key = 'd111ba1d-a1f5-43a5-98c6-347e9c2729b2'  # Replace with your actual DMI API key\n",
    "\n",
    "# Memory and performance settings\n",
    "MAX_MEMORY_USAGE_GB = 14     # Increased for maximum performance\n",
    "MAX_THREADS = 8              # Increased for better parallelism\n",
    "RATE_LIMIT_DELAY = 0.5       # Reduced for faster data collection\n",
    "MAX_RETRIES = 100             # High number of retries for resilience\n",
    "RETRY_DELAY = 20             # Reduced initial retry delay\n",
    "EXPONENTIAL_BACKOFF = True   # Still using exponential backoff to handle rate limits\n",
    "\n",
    "# Define the output directory\n",
    "output_dir = './dmi_data_daily'  # Changed to relative path for portability\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the bounding box for Denmark [min_lon, min_lat, max_lon, max_lat]\n",
    "denmark_bbox = [7.253075, 54.303704, 13.321548, 57.809651]\n",
    "\n",
    "# Define the parameters to retrieve\n",
    "parameters = [\n",
    "    \"precip_past1h\"  # Precipitation in the last hour\n",
    "]\n",
    "\n",
    "# Function to save current state of all station data - MOVED UP before it's used\n",
    "def save_station_data(stations_dict, output_directory, current_parameter):\n",
    "    \"\"\"Save all station data to individual JSON files.\"\"\"\n",
    "    logger.info(\"Saving current data to files...\")\n",
    "    saved_count = 0\n",
    "    \n",
    "    # Create a parameters directory to store parameter-specific files\n",
    "    params_dir = os.path.join(output_directory, 'parameters')\n",
    "    os.makedirs(params_dir, exist_ok=True)\n",
    "    \n",
    "    # First, save a parameter-specific file (for recovery if needed)\n",
    "    param_file = os.path.join(params_dir, f'parameter_{current_parameter}.json')\n",
    "    stations_with_param = {}\n",
    "    for station_id, station_data in stations_dict.items():\n",
    "        if current_parameter in station_data.get('parameters', {}):\n",
    "            # Create a copy with only this parameter's data\n",
    "            param_station = {\n",
    "                'stationId': station_data.get('stationId'),\n",
    "                'name': station_data.get('name', ''),\n",
    "                'location': station_data.get('location', {}),\n",
    "                'parameters': {current_parameter: station_data['parameters'][current_parameter]}\n",
    "            }\n",
    "            stations_with_param[station_id] = param_station\n",
    "    \n",
    "    # Save the parameter-specific file\n",
    "    with open(param_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(stations_with_param, f)\n",
    "    \n",
    "    # Now save individual station files with all accumulated data\n",
    "    stations_dir = os.path.join(output_directory, 'stations')\n",
    "    os.makedirs(stations_dir, exist_ok=True)\n",
    "    \n",
    "    for station_id, station_data in tqdm(stations_dict.items(), desc=\"Saving station data\"):\n",
    "        if station_data.get('parameters'):\n",
    "            # Save to JSON file\n",
    "            filename = os.path.join(stations_dir, f'station_{station_id}.json')\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(station_data, f, indent=2)\n",
    "            saved_count += 1\n",
    "    \n",
    "    logger.info(f\"Saved data for {saved_count} stations to {output_directory}\")\n",
    "    \n",
    "    # Also save a parameter progress file\n",
    "    progress_file = os.path.join(output_directory, 'parameter_progress.json')\n",
    "    with open(progress_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\n",
    "            'last_processed_parameter': current_parameter, \n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'stations_saved': saved_count\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    return saved_count\n",
    "\n",
    "# Calculate the time frame\n",
    "end_time = pd.Timestamp.now(tz='UTC')\n",
    "start_time = end_time - pd.DateOffset(years=30)  # Start 30 years ago\n",
    "datetime_str = f\"{start_time.isoformat()}/{end_time.isoformat()}\"\n",
    "\n",
    "# Function to retrieve all stations with retry logic\n",
    "def get_all_stations(api_key):\n",
    "    \"\"\"Retrieve all DMI stations, handling pagination and retries.\"\"\"\n",
    "    url = 'https://dmigw.govcloud.dk/v2/metObs/collections/station/items'\n",
    "    params = {'api-key': api_key, 'limit': '10000'}\n",
    "    stations = []\n",
    "    \n",
    "    retry_count = 0\n",
    "    while retry_count < MAX_RETRIES:\n",
    "        try:\n",
    "            logger.info(f\"Retrieving stations (attempt {retry_count + 1}/{MAX_RETRIES})...\")\n",
    "            r = requests.get(url, params=params, timeout=60)\n",
    "            r.raise_for_status()\n",
    "            json_data = r.json()\n",
    "            stations.extend(json_data['features'])\n",
    "            \n",
    "            next_link = next((link for link in json_data['links'] if link['rel'] == 'next'), None)\n",
    "            if next_link:\n",
    "                logger.info(f\"Found next page link, continuing pagination...\")\n",
    "                url = next_link['href']\n",
    "                params = {}  # Clear params for subsequent requests\n",
    "            else:\n",
    "                logger.info(f\"Station retrieval complete. Found {len(stations)} stations.\")\n",
    "                break\n",
    "                \n",
    "            # Add delay to avoid rate limiting\n",
    "            time.sleep(RATE_LIMIT_DELAY)\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            logger.error(f\"Error retrieving stations: {e}\")\n",
    "            retry_count += 1\n",
    "            if retry_count < MAX_RETRIES:\n",
    "                logger.info(f\"Retrying in {RETRY_DELAY} seconds...\")\n",
    "                time.sleep(RETRY_DELAY)\n",
    "            else:\n",
    "                logger.error(\"Maximum retry attempts reached. Proceeding with collected stations.\")\n",
    "                break\n",
    "    \n",
    "    return stations\n",
    "\n",
    "# Function to get data for a specific parameter with improved error handling\n",
    "def get_data_for_parameter(parameter_id, datetime_str, api_key, bbox=None, time_chunks=1):\n",
    "    \"\"\"Retrieve data for a specific parameter, with robust error handling and retry logic.\"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    # Parse start and end times\n",
    "    times = datetime_str.split('/')\n",
    "    start_time = pd.Timestamp(times[0])\n",
    "    end_time = pd.Timestamp(times[1])\n",
    "    \n",
    "    # Calculate the time delta for each chunk\n",
    "    total_days = (end_time - start_time).days\n",
    "    days_per_chunk = max(1, total_days // time_chunks)\n",
    "    \n",
    "    logger.info(f\"Splitting timeframe into {time_chunks} chunks of approximately {days_per_chunk} days each\")\n",
    "    \n",
    "    # Process each time chunk\n",
    "    for i in range(time_chunks):\n",
    "        chunk_start = start_time + pd.Timedelta(days=i * days_per_chunk)\n",
    "        chunk_end = start_time + pd.Timedelta(days=(i+1) * days_per_chunk) if i < time_chunks - 1 else end_time\n",
    "        chunk_datetime_str = f\"{chunk_start.isoformat()}/{chunk_end.isoformat()}\"\n",
    "        \n",
    "        logger.info(f\"Processing time chunk {i+1}/{time_chunks}: {chunk_start.date()} to {chunk_end.date()}\")\n",
    "        \n",
    "        # Set up the request parameters\n",
    "        url = 'https://dmigw.govcloud.dk/v2/metObs/collections/observation/items'\n",
    "        params = {\n",
    "            'api-key': api_key,\n",
    "            'datetime': chunk_datetime_str,\n",
    "            'parameterId': parameter_id,\n",
    "            'limit': '10000'  # Reduced limit to minimize server errors\n",
    "        }\n",
    "        \n",
    "        # Add bbox parameter if provided\n",
    "        if bbox:\n",
    "            params['bbox'] = f\"{bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]}\"\n",
    "        \n",
    "        # Variables to track pagination\n",
    "        offset = 0\n",
    "        max_offset = 490000  # Stay below the 500,000 limit\n",
    "        has_more = True\n",
    "        chunk_data = []\n",
    "        \n",
    "        # Retry loop\n",
    "        while has_more and offset < max_offset:\n",
    "            retry_count = 0\n",
    "            success = False\n",
    "            \n",
    "            while retry_count < MAX_RETRIES and not success:\n",
    "                try:\n",
    "                    # Add the offset parameter for pagination\n",
    "                    if offset > 0:\n",
    "                        params['offset'] = str(offset)\n",
    "                    \n",
    "                    logger.info(f\"Making request to: {url} with offset {offset} (attempt {retry_count + 1}/{MAX_RETRIES})\")\n",
    "                    r = requests.get(url, params=params, timeout=120)  # Increased timeout\n",
    "                    r.raise_for_status()\n",
    "                    json_data = r.json()\n",
    "                    \n",
    "                    if 'features' in json_data:\n",
    "                        batch_size = len(json_data['features'])\n",
    "                        logger.info(f\"Retrieved {batch_size} records in this batch\")\n",
    "                        chunk_data.extend(json_data['features'])\n",
    "                        \n",
    "                        # Save data periodically to avoid memory issues\n",
    "                        if len(chunk_data) >= 100000:  # Increased batch size before saving\n",
    "                            logger.info(\"Saving intermediate batch to avoid memory issues...\")\n",
    "                            save_parameter_batch(parameter_id, chunk_data, chunk_datetime_str)\n",
    "                            all_data.extend(chunk_data)  # Add to total count\n",
    "                            chunk_data = []  # Clear for next batch\n",
    "                            gc.collect()  # Force garbage collection\n",
    "                        \n",
    "                        # Check if we need to continue pagination\n",
    "                        if batch_size < int(params['limit']):\n",
    "                            has_more = False\n",
    "                        else:\n",
    "                            offset += batch_size\n",
    "                        \n",
    "                        success = True\n",
    "                    else:\n",
    "                        logger.warning(f\"No 'features' in response. Response: {json_data}\")\n",
    "                        has_more = False\n",
    "                        success = True\n",
    "                    \n",
    "                except requests.RequestException as e:\n",
    "                    error_msg = str(e)\n",
    "                    logger.error(f\"Error retrieving data: {error_msg}\")\n",
    "                    \n",
    "                    if hasattr(e, 'response') and e.response is not None:\n",
    "                        status_code = e.response.status_code\n",
    "                        response_text = e.response.text if hasattr(e.response, 'text') else \"No response text\"\n",
    "                        logger.error(f\"Status code: {status_code}, Response: {response_text}\")\n",
    "                        \n",
    "                        # Handle specific errors\n",
    "                        if status_code == 429:  # Too Many Requests\n",
    "                            logger.warning(\"Rate limit exceeded. Increasing wait time.\")\n",
    "                            time.sleep(RETRY_DELAY * 2)  # Double the retry delay\n",
    "                        elif status_code in [502, 503, 504]:  # Server errors\n",
    "                            logger.warning(f\"Server error {status_code}. Will retry.\")\n",
    "                        elif status_code == 400 and 'Offset cannot be greater than 500000' in response_text:\n",
    "                            logger.warning(\"Hit offset limit. Saving current batch and continuing with next time chunk.\")\n",
    "                            has_more = False\n",
    "                            success = True  # Exit retry loop but not while loop\n",
    "                    \n",
    "                    retry_count += 1\n",
    "                    if retry_count < MAX_RETRIES and not success:\n",
    "                        if EXPONENTIAL_BACKOFF:\n",
    "                            # Exponential backoff with jitter\n",
    "                            wait_time = min(RETRY_DELAY * (2 ** (retry_count - 1)) + (random.randint(0, 1000) / 1000), 600)\n",
    "                        else:\n",
    "                            wait_time = RETRY_DELAY * retry_count  # Linear backoff\n",
    "                        \n",
    "                        logger.info(f\"Retrying in {wait_time:.1f} seconds...\")\n",
    "                        time.sleep(wait_time)\n",
    "                    else:\n",
    "                        logger.error(\"Maximum retry attempts reached for this batch.\")\n",
    "                        has_more = False  # Stop trying this chunk\n",
    "                \n",
    "                # Add a delay between requests to avoid rate limiting\n",
    "                time.sleep(RATE_LIMIT_DELAY)\n",
    "        \n",
    "        # Save any remaining data from this chunk\n",
    "        if chunk_data:\n",
    "            logger.info(f\"Saving final batch of {len(chunk_data)} records for time chunk {i+1}\")\n",
    "            save_parameter_batch(parameter_id, chunk_data, chunk_datetime_str)\n",
    "            all_data.extend(chunk_data)\n",
    "            chunk_data = []\n",
    "            gc.collect()\n",
    "    \n",
    "    total_records = len(all_data)\n",
    "    logger.info(f\"Total records retrieved for {parameter_id}: {total_records}\")\n",
    "    return all_data\n",
    "\n",
    "# Function to save a batch of parameter data\n",
    "def save_parameter_batch(parameter_id, batch_data, datetime_str):\n",
    "    \"\"\"Save a batch of parameter data to a file.\"\"\"\n",
    "    if not batch_data:\n",
    "        return\n",
    "    \n",
    "    batch_size = len(batch_data)\n",
    "    logger.info(f\"Saving batch of {batch_size} records for {parameter_id}...\")\n",
    "    \n",
    "    # Create a batch directory\n",
    "    batch_dir = os.path.join(output_dir, 'parameter_batches', parameter_id)\n",
    "    os.makedirs(batch_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate a unique batch filename based on timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    batch_file = os.path.join(batch_dir, f'{parameter_id}_batch_{timestamp}.json')\n",
    "    \n",
    "    # Save the batch with metadata\n",
    "    batch_metadata = {\n",
    "        'parameter_id': parameter_id,\n",
    "        'datetime_range': datetime_str,\n",
    "        'record_count': batch_size,\n",
    "        'created_at': datetime.now().isoformat(),\n",
    "        'data': batch_data\n",
    "    }\n",
    "    \n",
    "    with open(batch_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(batch_metadata, f)\n",
    "    \n",
    "    logger.info(f\"Saved batch to {batch_file}\")\n",
    "    \n",
    "    # Update the parameter tracking file\n",
    "    update_parameter_tracking(parameter_id, batch_size, batch_file, datetime_str)\n",
    "\n",
    "# Function to update the parameter tracking file\n",
    "def update_parameter_tracking(parameter_id, batch_size, batch_file, datetime_str):\n",
    "    \"\"\"Update the parameter tracking file with new batch information.\"\"\"\n",
    "    tracking_file = os.path.join(output_dir, 'parameter_tracking.json')\n",
    "    tracking_data = {}\n",
    "    \n",
    "    # Load existing tracking data if it exists\n",
    "    if os.path.exists(tracking_file):\n",
    "        try:\n",
    "            with open(tracking_file, 'r', encoding='utf-8') as f:\n",
    "                tracking_data = json.load(f)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading tracking file: {e}\")\n",
    "    \n",
    "    # Update the tracking data\n",
    "    if parameter_id not in tracking_data:\n",
    "        tracking_data[parameter_id] = {\n",
    "            'total_records': 0,\n",
    "            'batches': []\n",
    "        }\n",
    "    \n",
    "    tracking_data[parameter_id]['total_records'] += batch_size\n",
    "    tracking_data[parameter_id]['batches'].append({\n",
    "        'file': os.path.basename(batch_file),\n",
    "        'records': batch_size,\n",
    "        'datetime_range': datetime_str,\n",
    "        'created_at': datetime.now().isoformat()\n",
    "    })\n",
    "    \n",
    "    # Save updated tracking data\n",
    "    with open(tracking_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(tracking_data, f, indent=2)\n",
    "\n",
    "# Function to combine parameter batches\n",
    "def combine_parameter_batches(parameter_id):\n",
    "    \"\"\"Combine all stored batches for a parameter into station data.\"\"\"\n",
    "    batch_dir = os.path.join(output_dir, 'parameter_batches', parameter_id)\n",
    "    if not os.path.exists(batch_dir):\n",
    "        logger.warning(f\"No batch directory found for parameter {parameter_id}\")\n",
    "        return {}\n",
    "    \n",
    "    # Dictionary to hold station data\n",
    "    stations_data = {}\n",
    "    \n",
    "    # List all batch files\n",
    "    batch_files = [f for f in os.listdir(batch_dir) if f.endswith('.json')]\n",
    "    if not batch_files:\n",
    "        logger.warning(f\"No batch files found for parameter {parameter_id}\")\n",
    "        return {}\n",
    "    \n",
    "    logger.info(f\"Found {len(batch_files)} batch files for parameter {parameter_id}\")\n",
    "    \n",
    "    # Process each batch file\n",
    "    for batch_file in tqdm(batch_files, desc=f\"Processing {parameter_id} batches\"):\n",
    "        try:\n",
    "            with open(os.path.join(batch_dir, batch_file), 'r', encoding='utf-8') as f:\n",
    "                batch_data = json.load(f)\n",
    "            \n",
    "            # Process each record in the batch\n",
    "            for record in batch_data['data']:\n",
    "                station_id = record['properties']['stationId']\n",
    "                \n",
    "                # Initialize station if not exists\n",
    "                if station_id not in stations_data:\n",
    "                    stations_data[station_id] = {\n",
    "                        'stationId': station_id,\n",
    "                        'parameters': {parameter_id: []}\n",
    "                    }\n",
    "                \n",
    "                # Initialize parameter if not exists\n",
    "                if parameter_id not in stations_data[station_id]['parameters']:\n",
    "                    stations_data[station_id]['parameters'][parameter_id] = []\n",
    "                \n",
    "                # Add the record\n",
    "                stations_data[station_id]['parameters'][parameter_id].append(record)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing batch file {batch_file}: {e}\")\n",
    "    \n",
    "    return stations_data\n",
    "\n",
    "# Main execution function\n",
    "def main():\n",
    "    # Retrieve all stations\n",
    "    logger.info(\"Starting DMI data collection process\")\n",
    "    all_stations = get_all_stations(api_key)\n",
    "\n",
    "    # Filter stations within the bounding box\n",
    "    filtered_stations = {}\n",
    "    for station in all_stations:\n",
    "        coords = station['geometry']['coordinates']\n",
    "        lon, lat = coords[0], coords[1]\n",
    "        if (denmark_bbox[0] <= lon <= denmark_bbox[2] and \n",
    "            denmark_bbox[1] <= lat <= denmark_bbox[3]):\n",
    "            station_id = station['properties']['stationId']\n",
    "            filtered_stations[station_id] = {\n",
    "                'stationId': station_id,\n",
    "                'name': station['properties'].get('name', ''),\n",
    "                'location': {\n",
    "                    'longitude': lon,\n",
    "                    'latitude': lat\n",
    "                },\n",
    "                'parameters': {}\n",
    "            }\n",
    "\n",
    "    logger.info(f\"Found {len(filtered_stations)} stations within the bounding box.\")\n",
    "\n",
    "    # Check for existing progress file\n",
    "    progress_file = os.path.join(output_dir, 'parameter_progress.json')\n",
    "    last_processed_idx = -1  # Start from the beginning by default\n",
    "    \n",
    "    if os.path.exists(progress_file):\n",
    "        try:\n",
    "            with open(progress_file, 'r') as f:\n",
    "                progress_data = json.load(f)\n",
    "                last_param = progress_data.get('last_processed_parameter')\n",
    "                if last_param in parameters:\n",
    "                    last_processed_idx = parameters.index(last_param)\n",
    "                    logger.info(f\"Resuming from parameter: {last_param} (index {last_processed_idx})\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading progress file: {e}\")\n",
    "    \n",
    "    # Create directory for parameter batches\n",
    "    batch_dir = os.path.join(output_dir, 'parameter_batches')\n",
    "    os.makedirs(batch_dir, exist_ok=True)\n",
    "    \n",
    "    # Process each parameter for all stations at once, starting after the last processed parameter\n",
    "    for i, parameter in enumerate(parameters[last_processed_idx+1:], start=last_processed_idx+1):\n",
    "        logger.info(f\"\\nRetrieving {parameter} data for all stations... ({i+1}/{len(parameters)})\")\n",
    "        \n",
    "        # Use smaller time chunks for better handling\n",
    "        time_chunks = 12  # Split into smaller chunks to reduce server load and handle errors better\n",
    "        \n",
    "        # First retrieve parameter data in batches\n",
    "        try:\n",
    "            logger.info(f\"Retrieving data for parameter {parameter} in batches...\")\n",
    "            get_data_for_parameter(parameter, datetime_str, api_key, denmark_bbox, time_chunks=time_chunks)\n",
    "            \n",
    "            # Now combine all batches for this parameter\n",
    "            logger.info(f\"Combining batches for parameter {parameter}...\")\n",
    "            parameter_stations = combine_parameter_batches(parameter)\n",
    "            \n",
    "            # Merge parameter data with existing station data\n",
    "            for station_id, station_data in parameter_stations.items():\n",
    "                if station_id in filtered_stations:\n",
    "                    # Add station metadata if it's a new station\n",
    "                    if 'name' not in station_data and 'name' in filtered_stations[station_id]:\n",
    "                        station_data['name'] = filtered_stations[station_id]['name']\n",
    "                    if 'location' not in station_data and 'location' in filtered_stations[station_id]:\n",
    "                        station_data['location'] = filtered_stations[station_id]['location']\n",
    "                    \n",
    "                    # Add parameter data to the station\n",
    "                    if parameter in station_data['parameters']:\n",
    "                        filtered_stations[station_id]['parameters'][parameter] = station_data['parameters'][parameter]\n",
    "            \n",
    "            # Save all station data after each parameter is processed\n",
    "            logger.info(f\"Completed processing parameter: {parameter}\")\n",
    "            save_station_data(filtered_stations, output_dir, parameter)\n",
    "            \n",
    "            # Free up memory\n",
    "            parameter_stations = None\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing parameter {parameter}: {e}\")\n",
    "            # Still save progress even if we encounter an error\n",
    "            save_station_data(filtered_stations, output_dir, f\"{parameter}_error\")\n",
    "\n",
    "    # Final save to ensure everything is written\n",
    "    logger.info(\"\\nPerforming final data save...\")\n",
    "    save_count = save_station_data(filtered_stations, output_dir, \"final\")\n",
    "    \n",
    "    logger.info(f\"Data extraction complete. Saved data for {save_count} stations.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "        logger.info(\"Program completed successfully\")\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Unexpected error: {e}\", exc_info=True)\n",
    "        logger.info(\"Program terminated with errors\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#=======================================================================================================#\n",
    "# This script extracts station information from the DMI API and saves it to a Parquet file. It also     #\n",
    "# creates a map visualization of the stations using Folium. The script first attempts to access the     #\n",
    "# stations endpoint directly. If that fails, it falls back to extracting unique station IDs from        #\n",
    "# observation data. The script handles potential issues with duplicate columns and missing coordinates. #\n",
    "# It also includes error handling for API requests and file saving. The map visualization is saved as   #\n",
    "# an HTML file in the specified directory. The script is designed to be run in a Python environment##   #\n",
    "# with the necessary libraries installed.                                                               #\n",
    "#=======================================================================================================#\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import folium\n",
    "import os\n",
    "\n",
    "# Replace this with your actual API key\n",
    "api_key = 'd111ba1d-a1f5-43a5-98c6-347e9c2729b2'  # insert your own key here\n",
    "\n",
    "# Method 1: Try to access stations endpoint directly\n",
    "stations_url = 'https://dmigw.govcloud.dk/v2/metObs/collections/station/items'\n",
    "\n",
    "def get_all_stations_method1():\n",
    "    \"\"\"\n",
    "    Attempt to get all stations using a dedicated station endpoint\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(stations_url, params={'api-key': api_key})\n",
    "        if response.status_code == 200:\n",
    "            stations_data = response.json()\n",
    "            \n",
    "            # Print the first feature to debug the structure\n",
    "            if 'features' in stations_data and stations_data['features']:\n",
    "                print(\"First station feature structure:\")\n",
    "                print(stations_data['features'][0])\n",
    "            \n",
    "            # Create a fresh DataFrame to avoid duplicates\n",
    "            stations_list = []\n",
    "            for feature in stations_data['features']:\n",
    "                station = {}\n",
    "                \n",
    "                # Extract basic info\n",
    "                station['id'] = feature.get('id')\n",
    "                station['feature_type'] = feature.get('type')\n",
    "                \n",
    "                # Extract geometry\n",
    "                if 'geometry' in feature:\n",
    "                    if 'coordinates' in feature['geometry']:\n",
    "                        coords = feature['geometry']['coordinates']\n",
    "                        if coords and len(coords) >= 2:\n",
    "                            station['longitude'] = coords[0]\n",
    "                            station['latitude'] = coords[1]\n",
    "                    if 'type' in feature['geometry']:\n",
    "                        station['geometry_type'] = feature['geometry']['type']\n",
    "                \n",
    "                # Extract properties\n",
    "                if 'properties' in feature:\n",
    "                    props = feature['properties']\n",
    "                    for key, value in props.items():\n",
    "                        # Rename 'type' to avoid duplicates\n",
    "                        if key == 'type':\n",
    "                            station['station_type'] = value\n",
    "                        else:\n",
    "                            station[key] = value\n",
    "                \n",
    "                stations_list.append(station)\n",
    "            \n",
    "            # Create DataFrame from our clean list\n",
    "            stations_df = pd.DataFrame(stations_list)\n",
    "            \n",
    "            return stations_df\n",
    "        else:\n",
    "            print(f\"Failed to access station endpoint. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing station endpoint: {e}\")\n",
    "        return None\n",
    "\n",
    "# Method 2: Extract unique stations from observation data\n",
    "def get_all_stations_method2():\n",
    "    \"\"\"\n",
    "    Get all stations by extracting unique station IDs from observation data\n",
    "    \"\"\"\n",
    "    dmi_url = 'https://dmigw.govcloud.dk/v2/metObs/collections/observation/items'\n",
    "    \n",
    "    try:\n",
    "        # Request with a high limit to get as many records as possible\n",
    "        params = {\n",
    "            'api-key': api_key,\n",
    "            'limit': '300000'  # Maximum allowed limit\n",
    "        }\n",
    "        \n",
    "        response = requests.get(dmi_url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        json_data = response.json()\n",
    "        \n",
    "        # Create a fresh DataFrame to avoid duplicates\n",
    "        stations_list = []\n",
    "        seen_station_ids = set()\n",
    "        \n",
    "        # Process each feature\n",
    "        for feature in json_data.get('features', []):\n",
    "            # Only process if it has properties and stationId\n",
    "            if 'properties' in feature and 'stationId' in feature['properties']:\n",
    "                station_id = feature['properties']['stationId']\n",
    "                \n",
    "                # Skip if we've already seen this station\n",
    "                if station_id in seen_station_ids:\n",
    "                    continue\n",
    "                \n",
    "                seen_station_ids.add(station_id)\n",
    "                \n",
    "                station = {'stationId': station_id}\n",
    "                \n",
    "                # Extract coordinates if available\n",
    "                if 'geometry' in feature and 'coordinates' in feature['geometry']:\n",
    "                    coords = feature['geometry']['coordinates']\n",
    "                    if coords and len(coords) >= 2:\n",
    "                        station['longitude'] = coords[0]\n",
    "                        station['latitude'] = coords[1]\n",
    "                \n",
    "                stations_list.append(station)\n",
    "        \n",
    "        # Create DataFrame from our clean list\n",
    "        stations_df = pd.DataFrame(stations_list)\n",
    "            \n",
    "        return stations_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting stations from observation data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Try both methods and use the one that works\n",
    "print(\"Attempting to get stations using Method 1...\")\n",
    "stations_df = get_all_stations_method1()\n",
    "if stations_df is None or stations_df.empty:\n",
    "    print(\"Falling back to method 2...\")\n",
    "    stations_df = get_all_stations_method2()\n",
    "\n",
    "if stations_df is not None and not stations_df.empty:\n",
    "    print(f\"Successfully retrieved {len(stations_df)} stations\")\n",
    "    print(\"\\nColumns in the DataFrame:\")\n",
    "    print(stations_df.columns.tolist())\n",
    "    print(\"\\nFirst 10 stations:\")\n",
    "    print(stations_df.head(10))\n",
    "    \n",
    "    # Check for duplicate column names\n",
    "    if len(stations_df.columns) != len(set(stations_df.columns)):\n",
    "        duplicate_cols = [col for col in stations_df.columns if list(stations_df.columns).count(col) > 1]\n",
    "        print(f\"Warning: Found duplicate columns: {duplicate_cols}\")\n",
    "    \n",
    "    # Save to Parquet in the specified directory\n",
    "    output_dir = '/Users/maks/Documents/GitHub/aba_flooding/dmi_data_daily'\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    output_path = os.path.join(output_dir, 'dmi_stations.parquet')\n",
    "    \n",
    "    try:\n",
    "        # Save to parquet\n",
    "        stations_df.to_parquet(output_path, index=False)\n",
    "        print(f\"\\nSaved station information to '{output_path}'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to Parquet: {e}\")\n",
    "        print(\"Detailed column information for debugging:\")\n",
    "        for i, col in enumerate(stations_df.columns):\n",
    "            print(f\"{i}: {col} - type: {stations_df[col].dtype}\")\n",
    "        \n",
    "        print(\"\\nPlease fix the column issues and try again.\")\n",
    "    \n",
    "    # Create a map visualization if coordinates are available\n",
    "    if 'longitude' in stations_df.columns and 'latitude' in stations_df.columns:\n",
    "        # Filter out rows with missing coordinates\n",
    "        map_df = stations_df.dropna(subset=['longitude', 'latitude'])\n",
    "        \n",
    "        if len(map_df) > 0:\n",
    "            # Calculate the center of the map\n",
    "            center_lat = map_df['latitude'].mean()\n",
    "            center_lon = map_df['longitude'].mean()\n",
    "            \n",
    "            # Create a map\n",
    "            m = folium.Map(location=[center_lat, center_lon], zoom_start=7)\n",
    "            \n",
    "            # Add markers for each station\n",
    "            for _, row in map_df.iterrows():\n",
    "                # Create popup with station info\n",
    "                popup_html = f\"\"\"\n",
    "                <b>Station ID:</b> {row.get('stationId', 'N/A')}<br>\n",
    "                \"\"\"\n",
    "                \n",
    "                # Add name if available\n",
    "                if 'name' in row and pd.notna(row['name']):\n",
    "                    popup_html += f\"<b>Name:</b> {row['name']}<br>\"\n",
    "                \n",
    "                # Add station type if available\n",
    "                if 'station_type' in row and pd.notna(row['station_type']):\n",
    "                    popup_html += f\"<b>Type:</b> {row['station_type']}<br>\"\n",
    "                \n",
    "                folium.Marker(\n",
    "                    location=[row['latitude'], row['longitude']],\n",
    "                    popup=folium.Popup(popup_html, max_width=300),\n",
    "                    icon=folium.Icon(color='blue')\n",
    "                ).add_to(m)\n",
    "            \n",
    "            # Save the map to the same directory\n",
    "            map_path = os.path.join(output_dir, 'dmi_stations_map.html')\n",
    "            m.save(map_path)\n",
    "            print(f\"Created map visualization in '{map_path}'\")\n",
    "else:\n",
    "    print(\"Failed to retrieve station information using both methods.\n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "#=======================================================================================================#\n",
    "# This script extracts station information from the DMI API and saves it to a Parquet file. It also     #\n",
    "# creates a map visualization of the stations using Folium. The script first attempts to access the     #\n",
    "# stations endpoint directly. If that fails, it falls back to extracting unique station IDs from        #\n",
    "# observation data. The script handles potential issues with duplicate columns and missing coordinates. #\n",
    "# It also includes error handling for API requests and file saving. The map visualization is saved as   #\n",
    "# an HTML file in the specified directory. The script is designed to be run in a Python environment##   #\n",
    "# with the necessary libraries installed.                                                               #\n",
    "#=======================================================================================================#\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import folium\n",
    "import os\n",
    "\n",
    "# Replace this with your actual API key\n",
    "api_key = 'd111ba1d-a1f5-43a5-98c6-347e9c2729b2'  # insert your own key here\n",
    "\n",
    "# Method 1: Try to access stations endpoint directly\n",
    "stations_url = 'https://dmigw.govcloud.dk/v2/metObs/collections/station/items'\n",
    "\n",
    "def get_all_stations_method1():\n",
    "    \"\"\"\n",
    "    Attempt to get all stations using a dedicated station endpoint\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(stations_url, params={'api-key': api_key})\n",
    "        if response.status_code == 200:\n",
    "            stations_data = response.json()\n",
    "            \n",
    "            # Print the first feature to debug the structure\n",
    "            if 'features' in stations_data and stations_data['features']:\n",
    "                print(\"First station feature structure:\")\n",
    "                print(stations_data['features'][0])\n",
    "            \n",
    "            # Create a fresh DataFrame to avoid duplicates\n",
    "            stations_list = []\n",
    "            for feature in stations_data['features']:\n",
    "                station = {}\n",
    "                \n",
    "                # Extract basic info\n",
    "                station['id'] = feature.get('id')\n",
    "                station['feature_type'] = feature.get('type')\n",
    "                \n",
    "                # Extract geometry\n",
    "                if 'geometry' in feature:\n",
    "                    if 'coordinates' in feature['geometry']:\n",
    "                        coords = feature['geometry']['coordinates']\n",
    "                        if coords and len(coords) >= 2:\n",
    "                            station['longitude'] = coords[0]\n",
    "                            station['latitude'] = coords[1]\n",
    "                    if 'type' in feature['geometry']:\n",
    "                        station['geometry_type'] = feature['geometry']['type']\n",
    "                \n",
    "                # Extract properties\n",
    "                if 'properties' in feature:\n",
    "                    props = feature['properties']\n",
    "                    for key, value in props.items():\n",
    "                        # Rename 'type' to avoid duplicates\n",
    "                        if key == 'type':\n",
    "                            station['station_type'] = value\n",
    "                        else:\n",
    "                            station[key] = value\n",
    "                \n",
    "                stations_list.append(station)\n",
    "            \n",
    "            # Create DataFrame from our clean list\n",
    "            stations_df = pd.DataFrame(stations_list)\n",
    "            \n",
    "            return stations_df\n",
    "        else:\n",
    "            print(f\"Failed to access station endpoint. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing station endpoint: {e}\")\n",
    "        return None\n",
    "\n",
    "# Method 2: Extract unique stations from observation data\n",
    "def get_all_stations_method2():\n",
    "    \"\"\"\n",
    "    Get all stations by extracting unique station IDs from observation data\n",
    "    \"\"\"\n",
    "    dmi_url = 'https://dmigw.govcloud.dk/v2/metObs/collections/observation/items'\n",
    "    \n",
    "    try:\n",
    "        # Request with a high limit to get as many records as possible\n",
    "        params = {\n",
    "            'api-key': api_key,\n",
    "            'limit': '300000'  # Maximum allowed limit\n",
    "        }\n",
    "        \n",
    "        response = requests.get(dmi_url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        json_data = response.json()\n",
    "        \n",
    "        # Create a fresh DataFrame to avoid duplicates\n",
    "        stations_list = []\n",
    "        seen_station_ids = set()\n",
    "        \n",
    "        # Process each feature\n",
    "        for feature in json_data.get('features', []):\n",
    "            # Only process if it has properties and stationId\n",
    "            if 'properties' in feature and 'stationId' in feature['properties']:\n",
    "                station_id = feature['properties']['stationId']\n",
    "                \n",
    "                # Skip if we've already seen this station\n",
    "                if station_id in seen_station_ids:\n",
    "                    continue\n",
    "                \n",
    "                seen_station_ids.add(station_id)\n",
    "                \n",
    "                station = {'stationId': station_id}\n",
    "                \n",
    "                # Extract coordinates if available\n",
    "                if 'geometry' in feature and 'coordinates' in feature['geometry']:\n",
    "                    coords = feature['geometry']['coordinates']\n",
    "                    if coords and len(coords) >= 2:\n",
    "                        station['longitude'] = coords[0]\n",
    "                        station['latitude'] = coords[1]\n",
    "                \n",
    "                stations_list.append(station)\n",
    "        \n",
    "        # Create DataFrame from our clean list\n",
    "        stations_df = pd.DataFrame(stations_list)\n",
    "            \n",
    "        return stations_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting stations from observation data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Try both methods and use the one that works\n",
    "print(\"Attempting to get stations using Method 1...\")\n",
    "stations_df = get_all_stations_method1()\n",
    "if stations_df is None or stations_df.empty:\n",
    "    print(\"Falling back to method 2...\")\n",
    "    stations_df = get_all_stations_method2()\n",
    "\n",
    "if stations_df is not None and not stations_df.empty:\n",
    "    print(f\"Successfully retrieved {len(stations_df)} stations\")\n",
    "    print(\"\\nColumns in the DataFrame:\")\n",
    "    print(stations_df.columns.tolist())\n",
    "    print(\"\\nFirst 10 stations:\")\n",
    "    print(stations_df.head(10))\n",
    "    \n",
    "    # Check for duplicate column names\n",
    "    if len(stations_df.columns) != len(set(stations_df.columns)):\n",
    "        duplicate_cols = [col for col in stations_df.columns if list(stations_df.columns).count(col) > 1]\n",
    "        print(f\"Warning: Found duplicate columns: {duplicate_cols}\")\n",
    "    \n",
    "    # Save to Parquet in the specified directory\n",
    "    output_dir = '/Users/maks/Documents/GitHub/aba_flooding/dmi_data_daily'\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    output_path = os.path.join(output_dir, 'dmi_stations.parquet')\n",
    "    \n",
    "    try:\n",
    "        # Save to parquet\n",
    "        stations_df.to_parquet(output_path, index=False)\n",
    "        print(f\"\\nSaved station information to '{output_path}'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to Parquet: {e}\")\n",
    "        print(\"Detailed column information for debugging:\")\n",
    "        for i, col in enumerate(stations_df.columns):\n",
    "            print(f\"{i}: {col} - type: {stations_df[col].dtype}\")\n",
    "        \n",
    "        print(\"\\nPlease fix the column issues and try again.\")\n",
    "    \n",
    "    # Create a map visualization if coordinates are available\n",
    "    if 'longitude' in stations_df.columns and 'latitude' in stations_df.columns:\n",
    "        # Filter out rows with missing coordinates\n",
    "        map_df = stations_df.dropna(subset=['longitude', 'latitude'])\n",
    "        \n",
    "        if len(map_df) > 0:\n",
    "            # Calculate the center of the map\n",
    "            center_lat = map_df['latitude'].mean()\n",
    "            center_lon = map_df['longitude'].mean()\n",
    "            \n",
    "            # Create a map\n",
    "            m = folium.Map(location=[center_lat, center_lon], zoom_start=7)\n",
    "            \n",
    "            # Add markers for each station\n",
    "            for _, row in map_df.iterrows():\n",
    "                # Create popup with station info\n",
    "                popup_html = f\"\"\"\n",
    "                <b>Station ID:</b> {row.get('stationId', 'N/A')}<br>\n",
    "                \"\"\"\n",
    "                \n",
    "                # Add name if available\n",
    "                if 'name' in row and pd.notna(row['name']):\n",
    "                    popup_html += f\"<b>Name:</b> {row['name']}<br>\"\n",
    "                \n",
    "                # Add station type if available\n",
    "                if 'station_type' in row and pd.notna(row['station_type']):\n",
    "                    popup_html += f\"<b>Type:</b> {row['station_type']}<br>\"\n",
    "                \n",
    "                folium.Marker(\n",
    "                    location=[row['latitude'], row['longitude']],\n",
    "                    popup=folium.Popup(popup_html, max_width=300),\n",
    "                    icon=folium.Icon(color='blue')\n",
    "                ).add_to(m)\n",
    "            \n",
    "            # Save the map to the same directory\n",
    "            map_path = os.path.join(output_dir, 'dmi_stations_map.html')\n",
    "            m.save(map_path)\n",
    "            print(f\"Created map visualization in '{map_path}'\")\n",
    "else:\n",
    "    print(\"Failed to retrieve station information using both methods.\")\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "#=================================================================================================#\n",
    "# This script processes JSON files containing precipitation data, extracts relevant information,  #\n",
    "# and saves the data into a Parquet file. It also logs the processing steps and statistics.       #\n",
    "#=================================================================================================#\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"precipitation_processing.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def process_json_files(folder_path):\n",
    "    # Get all json files in the folder\n",
    "    json_files = glob.glob(os.path.join(folder_path, \"*.json\"))\n",
    "    logger.info(f\"Found {len(json_files)} JSON files to process\")\n",
    "    \n",
    "    # Create a dictionary to store all data\n",
    "    all_data = {}\n",
    "    total_records = 0\n",
    "    \n",
    "    # Process each file with a progress bar\n",
    "    for file_path in tqdm(json_files, desc=\"Processing JSON files\"):\n",
    "        logger.info(f\"Processing file: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        try:\n",
    "            # Load the JSON file\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            file_records = 0\n",
    "            # Extract the data points\n",
    "            for item in data['data']:\n",
    "                # Get timestamp, station ID, and value\n",
    "                timestamp = item['properties']['observed']\n",
    "                station_id = item['properties']['stationId']\n",
    "                value = item['properties']['value']\n",
    "                \n",
    "                # Convert timestamp to datetime object\n",
    "                timestamp = datetime.strptime(timestamp, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                \n",
    "                # Store in dictionary: all_data[timestamp][station_id] = value\n",
    "                if timestamp not in all_data:\n",
    "                    all_data[timestamp] = {}\n",
    "                \n",
    "                all_data[timestamp][station_id] = value\n",
    "                file_records += 1\n",
    "            \n",
    "            total_records += file_records\n",
    "            logger.info(f\"Extracted {file_records} records from {os.path.basename(file_path)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {os.path.basename(file_path)}: {str(e)}\")\n",
    "    \n",
    "    logger.info(f\"Total records processed: {total_records}\")\n",
    "    logger.info(\"Converting to DataFrame...\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame.from_dict(all_data, orient='index')\n",
    "    \n",
    "    # Sort index (timestamps) chronologically\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    logger.info(f\"DataFrame created with shape: {df.shape}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"/Users/maks/Documents/GitHub/aba_flooding/dmi_data_daily/parameter_batches/precip_past1h_test\"\n",
    "    \n",
    "    logger.info(\"Starting precipitation data processing\")\n",
    "    \n",
    "    # Process the files\n",
    "    precipitation_df = process_json_files(folder_path)\n",
    "    \n",
    "    # Get some stats\n",
    "    memory_usage_mb = precipitation_df.memory_usage(deep=True).sum() / 1048576\n",
    "    num_stations = len(precipitation_df.columns)\n",
    "    num_timestamps = len(precipitation_df)\n",
    "    \n",
    "    # Save to Parquet\n",
    "    logger.info(\"Saving data to Parquet file...\")\n",
    "    precipitation_df.to_parquet(\"precipitation_data_test.parquet\")\n",
    "    \n",
    "    logger.info(f\"Processing complete. Data saved to precipitation_data_test.parquet\")\n",
    "    logger.info(f\"DataFrame shape: {precipitation_df.shape}\")\n",
    "    logger.info(f\"Number of timestamps: {num_timestamps}\")\n",
    "    logger.info(f\"Number of stations: {num_stations}\")\n",
    "    logger.info(f\"DataFrame memory usage: {memory_usage_mb:.2f} MB\")\n",
    "    logger.info(f\"Date range: {precipitation_df.index.min()} to {precipitation_df.index.max()}\")\n",
    "    \n",
    "    print(\"\\nSample of the data:\")\n",
    "    print(precipitation_df.head())             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting sediment data ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explain process of extracting sediment data via QGIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import aba_flooding.perculation_mapping as pm\n",
    "import aba_flooding.geo_utils as gu\n",
    "# import perculation_mapping as pm\n",
    "# import geo_utils as gu\n",
    "from shapely.geometry import Point, Polygon\n",
    "from scipy.spatial import Voronoi\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "###########################################\n",
    "# SECTION 1: GEOGRAPHIC DATA PROCESSING   #\n",
    "###########################################\n",
    "\n",
    "def voronoi_finite_polygons_2d(vor, radius=None):\n",
    "    \"\"\"Convert Voronoi diagram to finite polygons.\"\"\"\n",
    "    if vor.points.shape[1] != 2:\n",
    "        raise ValueError(\"Requires 2D input\")\n",
    "    \n",
    "    new_regions = []\n",
    "    new_vertices = vor.vertices.tolist()\n",
    "    \n",
    "    center = vor.points.mean(axis=0)\n",
    "    radius = np.ptp(vor.points, axis=0).max() * 2 if radius is None else radius\n",
    "    \n",
    "    # Construct a map of all ridges for a given point\n",
    "    all_ridges = {}\n",
    "    for (p1, p2), (v1, v2) in zip(vor.ridge_points, vor.ridge_vertices):\n",
    "        all_ridges.setdefault(p1, []).append((p2, v1, v2))\n",
    "        all_ridges.setdefault(p2, []).append((p1, v1, v2))\n",
    "    \n",
    "    # Reconstruct infinite regions\n",
    "    for p1, region in enumerate(vor.point_region):\n",
    "        # Skip points that don't have any ridges\n",
    "        if p1 not in all_ridges:\n",
    "            print(f\"Skipping point {p1} which has no ridges\")\n",
    "            continue\n",
    "            \n",
    "        vertices = vor.regions[region]\n",
    "        if all(v >= 0 for v in vertices):\n",
    "            # Finite region\n",
    "            new_regions.append(vertices)\n",
    "            continue\n",
    "        \n",
    "        # Reconstruct a non-finite region\n",
    "        ridges = all_ridges[p1]\n",
    "        new_region = [v for v in vertices if v >= 0]\n",
    "        \n",
    "        for p2, v1, v2 in ridges:\n",
    "            if v2 < 0:\n",
    "                v1, v2 = v2, v1\n",
    "            if v1 >= 0:\n",
    "                # Finite ridge\n",
    "                continue\n",
    "            \n",
    "            # Infinite ridge\n",
    "            t = vor.points[p2] - vor.points[p1]  # tangent\n",
    "            t /= np.linalg.norm(t)\n",
    "            n = np.array([-t[1], t[0]])  # normal\n",
    "            \n",
    "            midpoint = vor.points[[p1, p2]].mean(axis=0)\n",
    "            direction = np.sign(np.dot(midpoint - center, n)) * n\n",
    "            far_point = vor.vertices.mean(axis=0) + direction * radius\n",
    "            \n",
    "            new_region.append(len(new_vertices))\n",
    "            new_vertices.append(far_point.tolist())\n",
    "        \n",
    "        # Sort region counterclockwise\n",
    "        vs = np.asarray([new_vertices[v] for v in new_region])\n",
    "        c = vs.mean(axis=0)\n",
    "        angles = np.arctan2(vs[:,1] - c[1], vs[:,0] - c[0])\n",
    "        new_region = np.array(new_region)[np.argsort(angles)]\n",
    "        \n",
    "        new_regions.append(new_region.tolist())\n",
    "    \n",
    "    return new_regions, np.asarray(new_vertices)\n",
    "\n",
    "def create_precipitation_coverage(denmark_gdf):\n",
    "    \"\"\"\n",
    "    Create Voronoi polygons for precipitation stations that cover Denmark without overlap.\n",
    "    \n",
    "    Called by: create_full_coverage()\n",
    "    Calls: voronoi_finite_polygons_2d()\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load dmi station data - stations are rows with longitude and latitude columns\n",
    "        print(f\"Loading dmi station data from data/raw/dmi_stations.parquet...\")\n",
    "        station_data = pd.read_parquet('data/raw/dmi_stations.parquet')\n",
    "        print(f\"Loaded coordinate data of the station with {len(station_data)} stations\")\n",
    "        \n",
    "        # Determine the station ID column\n",
    "        if 'stationId' in station_data.columns:\n",
    "            id_column = 'stationId' \n",
    "        else:\n",
    "            print(\"WARNING: No 'stationId' column found, using first column as ID\")   \n",
    "            id_column = station_data.columns[0]  # Use the first column as ID \n",
    "        \n",
    "        # Find longitude and latitude columns\n",
    "        lon_col = next((col for col in station_data.columns if col.lower() in ['longitude', 'lon', 'long']), None)\n",
    "        lat_col = next((col for col in station_data.columns if col.lower() in ['latitude', 'lat']), None)\n",
    "        \n",
    "        if lon_col is None or lat_col is None:\n",
    "            raise ValueError(f\"Could not identify longitude and latitude columns. Available columns: {station_data.columns.tolist()}\")\n",
    "        print(f\"Using column '{lon_col}' for longitude, '{lat_col}' for latitude, and '{id_column}' for station IDs\")\n",
    "        \n",
    "        # Create GeoDataFrame for stations\n",
    "        print(\"Creating GeoDataFrame for stations containing their coordinates\")\n",
    "        stations_gdf = gpd.GeoDataFrame(\n",
    "            station_data,\n",
    "            geometry=gpd.points_from_xy(station_data[lon_col], station_data[lat_col]),\n",
    "            crs=\"EPSG:4326\"  # WGS84 ellipsoid is a coordinate system used in Google Earth and GSP systems\n",
    "        )\n",
    "        \n",
    "        # Reproject to match map projection since the Voronoi diagram is in EPSG:3857\n",
    "        print(\"Reprojecting to Web Mercator EPSG:3857 for Voronoi diagram calculations\")\n",
    "        stations_gdf = stations_gdf.to_crs(\"EPSG:3857\")  # Web Mercator\n",
    "        # This EPSG:3857 makes the the X/Y coordinates in meters, which is suitable for Voronoi diagram calculations\n",
    "        # Basically it makes it square, so the Voronoi diagram is not distorted by the curvature of the earth\n",
    "        \n",
    "        # Print information about Denmark boundary\n",
    "        # print(f\"Denmark GDF info: {denmark_gdf.shape}\")\n",
    "        # print(f\"Denmark GDF columns: {denmark_gdf.columns.tolist()}\")\n",
    "        # print(f\"Denmark CRS: {denmark_gdf.crs}\")\n",
    "        \n",
    "        # Create Voronoi diagram\n",
    "        print(\"Creating Voronoi diagram...\")\n",
    "        coords = np.array([(p.x, p.y) for p in stations_gdf.geometry])\n",
    "        print(f\"Number of station coordinates: {len(coords)}\")\n",
    "        \n",
    "        # Check for duplicate or very close points\n",
    "        _, unique_indices = np.unique(np.round(coords, decimals=5), axis=0, return_index=True)\n",
    "        if len(unique_indices) < len(coords):\n",
    "            print(f\"WARNING: Found {len(coords) - len(unique_indices)} potential duplicate stations. Using only unique locations.\")\n",
    "            coords = coords[np.sort(unique_indices)]\n",
    "            # Adjust stations_gdf to match unique points\n",
    "            stations_gdf = stations_gdf.iloc[np.sort(unique_indices)].copy()\n",
    "        \n",
    "        # Get Denmark boundary for clipping\n",
    "        boundary = denmark_gdf.geometry.union_all().bounds\n",
    "        print(f\"Denmark bounds:\")\n",
    "        print(f\"\\tSW corner city: {boundary[0]}, {boundary[1]}\")\n",
    "        print(f\"\\tNE corner city: {boundary[2]}, {boundary[3]}\")\n",
    "\n",
    "        boundary_width = boundary[2] - boundary[0]\n",
    "        boundary_height = boundary[3] - boundary[1]\n",
    "\n",
    "        # Add corner points to ensure complete coverage\n",
    "        corner_points = [\n",
    "            [boundary[0] - boundary_width, boundary[1] - boundary_height],\n",
    "            [boundary[2] + boundary_width, boundary[1] - boundary_height],\n",
    "            [boundary[0] - boundary_width, boundary[3] + boundary_height],\n",
    "            [boundary[2] + boundary_width, boundary[3] + boundary_height]\n",
    "        ]\n",
    "        \n",
    "        all_points = np.vstack([coords, corner_points])\n",
    "        print(f\"Total points for Voronoi (including corners): {len(all_points)}\")\n",
    "        \n",
    "        try:\n",
    "            vor = Voronoi(all_points)\n",
    "            print(f\"Voronoi diagram created with {len(vor.points)} points and {len(vor.vertices)} vertices\")\n",
    "        except Exception as vor_error:\n",
    "            print(f\"ERROR creating Voronoi diagram: {vor_error}\")\n",
    "            # Add jitter to points to avoid collinearity issues\n",
    "            jitter = np.random.normal(0, 0.00001, all_points.shape)\n",
    "            all_points = all_points + jitter\n",
    "            print(\"Added small jitter to points to avoid numerical issues, retrying...\")\n",
    "            vor = Voronoi(all_points)\n",
    "        \n",
    "        # Get Voronoi polygons\n",
    "        print(\"Converting Voronoi diagram to polygons...\")\n",
    "        regions, vertices = voronoi_finite_polygons_2d(vor)\n",
    "        print(f\"Created {len(regions)} Voronoi regions\")\n",
    "        \n",
    "        # Create clipped polygons for each station \n",
    "        # this is because the Voronoi polygons can be infinite. So we need to clip them to the Denmark boundary\n",
    "        print(\"Creating clipped polygons so that they are within the Denmark boundary...\")\n",
    "        voronoi_polygons = []\n",
    "        valid_station_ids = []\n",
    "        \n",
    "        for i, region in enumerate(regions):\n",
    "            if i < len(coords):  # Skip corner points\n",
    "                try:\n",
    "                    polygon = Polygon([vertices[v] for v in region])\n",
    "                    # Clip polygon to Denmark boundary\n",
    "                    clipped_polygon = polygon.intersection(denmark_gdf.geometry.union_all())\n",
    "                    if not clipped_polygon.is_empty:\n",
    "                        voronoi_polygons.append(clipped_polygon)\n",
    "                        valid_station_ids.append(stations_gdf.iloc[i][id_column])\n",
    "                except Exception as poly_error:\n",
    "                    print(f\"ERROR creating polygon for region {i}: {poly_error}\")\n",
    "                    continue\n",
    "        \n",
    "        print(f\"Created {len(voronoi_polygons)} valid polygons\")\n",
    "        \n",
    "        # Create GeoDataFrame with coverage areas, meaning that the polygons are the coverage areas of the stations\n",
    "        coverage_gdf = gpd.GeoDataFrame(\n",
    "            {'station_id': valid_station_ids},\n",
    "            geometry=voronoi_polygons,\n",
    "            crs=stations_gdf.crs\n",
    "        )\n",
    "        \n",
    "        # TODO: Add avg_precipitation data to coverage areas if available\n",
    "        print(\"Adding avg preciptation to the station data of the polygons...\")\n",
    "        # load in the precipitation data\n",
    "        precipitation_data = pd.read_parquet('data/raw/precipitation_imputed_data.parquet')\n",
    "        precipitation_data = precipitation_data.clip(lower=0, upper=100) \n",
    "        # drop nans\n",
    "        # precipitation_data.dropna(inplace=True) # inplace means that \n",
    "\n",
    "        # precipitation_data has columns indexed by station IDs with the mm values\n",
    "        avg_prec = precipitation_data.mean(axis=0, skipna=True)\n",
    "        print(f\"Highest average precipitation: {avg_prec.max()}\")\n",
    "        print(f\"Lowest average precipitation: {avg_prec.min()}\")\n",
    "\n",
    "        # check the avg_precipitation values as a histogram\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.title(\"Average Hourly Precipitation Histogram\")\n",
    "        plt.xlabel(\"Average Hourly Precipitation (mm)\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.hist(avg_prec, bins=30, color='blue', alpha=0.7)\n",
    "        plt.grid(axis='y', alpha=0.75)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"outputs/plots/avg_precipitation_histogram.png\")\n",
    "        \n",
    "        # Return the coverage GeoDataFrame and stations GeoDataFrame\n",
    "        # File saving is handled in create_full_coverage() to avoid duplication\n",
    "        return coverage_gdf, stations_gdf\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR creating precipitation coverage: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "def create_full_coverage():\n",
    "    \"\"\"\n",
    "    Create coverage areas for precipitation stations across Denmark.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (GeoJSONDataSource, GeoDataFrame, GeoDataFrame)\n",
    "        Coverage as GeoJSON source - contains geo data in GeoJSON format\n",
    "        coverage GeoDataFrame      - \n",
    "        stations GeoDataFrame\n",
    "    \n",
    "    Called by: main\n",
    "    Calls: create_precipitation_coverage(), gu.gdf_to_geojson()\n",
    "    \"\"\"\n",
    "    # Create directories for output files\n",
    "    os.makedirs(\"data/processed\", exist_ok=True)\n",
    "    os.makedirs(\"data/raw\", exist_ok=True)\n",
    "    \n",
    "    # Create a simplified Denmark boundary manually\n",
    "    print(\"Creating simplified Denmark boundary...\")\n",
    "    # Approximate Denmark bounding box in EPSG:4326 (WGS84)\n",
    "    # These coordinates represent a rough bounding box around Denmark\n",
    "    denmark_coords = [\n",
    "        (8.0, 54.5),   # Southwest \n",
    "        (8.0, 57.8),   # Northwest \n",
    "        (13.0, 57.8),  # Northeast \n",
    "        (13.0, 54.5),  # Southeast\n",
    "        (8.0, 54.5)    # to close the polygon\n",
    "    ]\n",
    "    \n",
    "    # Create a polygon and convert to GeoDataFrame\n",
    "    denmark_polygon = Polygon(denmark_coords)\n",
    "    denmark_polygon_gdf = gpd.GeoDataFrame(\n",
    "        {'name': ['Denmark']}, \n",
    "        geometry=[denmark_polygon], \n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(epsg=3857)\n",
    "    print(\"Using simplified Denmark boundary\")\n",
    "\n",
    "    # Create station coverage areas\n",
    "    print(\"\\nCreating station coverage areas\")\n",
    "    coverage_geojson_gdf, stations_gdf = create_precipitation_coverage(denmark_polygon_gdf)\n",
    "    \n",
    "    if coverage_geojson_gdf is not None and not coverage_geojson_gdf.empty:\n",
    "        print(f\"Successfully created polygon coverage GeoDataFrame with {len(coverage_geojson_gdf)} polygons\")\n",
    "        \n",
    "        # Save the GeoJSON file using various methods as fallbacks\n",
    "        try:\n",
    "            coverage_geojson_gdf.to_file(\"data/raw/precipitation_coverage.geojson\", driver=\"GeoJSON\")\n",
    "            print(\"Saved coverage GeoDataFrame to data/raw/precipitation_coverage.geojson\")\n",
    "        except AttributeError as e:\n",
    "            if \"module 'pyogrio' has no attribute 'write_dataframe'\" in str(e):\n",
    "                print(\"ERROR saving 'precipitation_coverage.geojson' due to pyogrio error has no attribute 'write_dataframe'\") \n",
    "                try:\n",
    "                    # Try using fiona driver directly\n",
    "                    import fiona\n",
    "                    coverage_geojson_gdf.to_file(\n",
    "                        \"data/raw/precipitation_coverage.geojson\", \n",
    "                        driver=\"GeoJSON\",\n",
    "                        engine=\"fiona\"\n",
    "                    )\n",
    "                    print(\"Saved coverage GeoDataFrame using fiona engine\")\n",
    "                except Exception as fiona_error:\n",
    "                    print(f\"ERROR Fiona method failed also: {fiona_error}\")\n",
    "                    try:\n",
    "                        # Last resort: manually create GeoJSON\n",
    "                        import json\n",
    "                        geojson_dict = json.loads(gu.gdf_to_geojson(coverage_geojson_gdf))\n",
    "                        with open(\"data/raw/precipitation_coverage.geojson\", \"w\") as f:\n",
    "                            json.dump(geojson_dict, f)\n",
    "                        print(\"Saved coverage GeoDataFrame using manual JSON conversion\")\n",
    "                    except Exception as json_error:\n",
    "                        print(f\"ERROR Manual JSON conversion failed: {json_error}\")\n",
    "                        print(\"ERROR: Could not save precipitation coverage file\")\n",
    "            else:\n",
    "                print(f\"ERROR Could not save precipitation coverage areas: {e}\")\n",
    "        except Exception as general_error:\n",
    "            print(f\"ERROR Could not save precipitation coverage areas: {general_error}\")\n",
    "        \n",
    "        return coverage_geojson_gdf, stations_gdf\n",
    "    \n",
    "    else:\n",
    "        print(\"No valid coverage areas created. Skipping GeoJSON creation.\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# SECTION 2: SOIL AND SEDIMENT ANALYSIS   #\n",
    "###########################################\n",
    "\n",
    "def sediment_types_for_station(stationId, precipitationCoverageStations, sedimentCoverage):\n",
    "    \"\"\"\n",
    "    Get all soil types contained within a station area.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------    \n",
    "    stationId : str\n",
    "        Station ID to filter soil types\n",
    "    precipitationCoverageStations : geoDataFrame\n",
    "        GeoDataFrame containing precipitation coverage data\n",
    "    sedimentCoverage : geoDataFrame\n",
    "        GeoDataFrame containing sediment coverage data\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list : List of soil types within the station area or empty list if station not found\n",
    "    \n",
    "    Called by: load_process_data()\n",
    "    \"\"\"\n",
    "    # Debug information about the datasets\n",
    "    print(f\"Precipitation coverage CRS: {precipitationCoverageStations.crs}\")\n",
    "    print(f\"Sediment coverage CRS: {sedimentCoverage.crs}\")\n",
    "    \n",
    "    # Check if 'stationId' or 'station_id' column exists\n",
    "    id_column = 'station_id'\n",
    "    if 'stationId' in precipitationCoverageStations.columns:\n",
    "        id_column = 'stationId'\n",
    "    \n",
    "    print(f\"Using {id_column} to identify stations.\")\n",
    "    \n",
    "    # Get the stations matching the stationId\n",
    "    matching_stations = precipitationCoverageStations[precipitationCoverageStations[id_column] == stationId]\n",
    "    \n",
    "    # Check if we found any matching stations\n",
    "    if matching_stations.empty:\n",
    "        print(f\"Warning: No station found with ID {stationId}\")\n",
    "        return []\n",
    "    \n",
    "    # Get the geometry of the station\n",
    "    station_geometry = matching_stations.geometry.iloc[0]\n",
    "    \n",
    "    # Print geometry information for debugging\n",
    "    print(f\"Station geometry type: {station_geometry.geom_type}\")\n",
    "    print(f\"Station geometry bounds: {station_geometry.bounds}\")\n",
    "    \n",
    "    # Ensure both datasets use the same CRS\n",
    "    if precipitationCoverageStations.crs != sedimentCoverage.crs:\n",
    "        #print(f\"CRS mismatch! Reprojecting sediment coverage to {precipitationCoverageStations.crs}\")\n",
    "        sedimentCoverage = sedimentCoverage.to_crs(precipitationCoverageStations.crs)\n",
    "    \n",
    "    # Check if the geometries are valid\n",
    "    if not station_geometry.is_valid:\n",
    "        print(\"Station geometry is invalid! Attempting to fix...\")\n",
    "        station_geometry = station_geometry.buffer(0)\n",
    "    \n",
    "    # Use buffer to account for possible precision issues\n",
    "    # This creates a small buffer around the station geometry to increase chances of intersection\n",
    "    buffered_geometry = station_geometry.buffer(1)  # 1 meter buffer\n",
    "\n",
    "    # Try with the buffered geometry first\n",
    "    sediment_types_buffered = sedimentCoverage[sedimentCoverage.intersects(buffered_geometry)]\n",
    "    \n",
    "    if not sediment_types_buffered.empty:\n",
    "        print(f\"Found {len(sediment_types_buffered)} sediment features using buffered geometry\")\n",
    "        sediment_types = sediment_types_buffered\n",
    "    else:\n",
    "        # If buffered approach fails, try with original geometry\n",
    "        sediment_types = sedimentCoverage[sedimentCoverage.intersects(station_geometry)]\n",
    "        if sediment_types.empty:\n",
    "            # Check if any sediment polygons are nearby\n",
    "            # This helps diagnose if the issue is with projection or data\n",
    "            buffer_distance = 1000  # 1 km\n",
    "            large_buffer = station_geometry.buffer(buffer_distance)\n",
    "            nearby_sediments = sedimentCoverage[sedimentCoverage.intersects(large_buffer)]\n",
    "            \n",
    "            if not nearby_sediments.empty:\n",
    "                print(f\"Found {len(nearby_sediments)} sediment features within {buffer_distance}m\")\n",
    "                print(\"The issue might be with projection or precision\")\n",
    "            else:\n",
    "                print(f\"No sediment features found even within {buffer_distance}m\")\n",
    "                print(\"The station might be outside the sediment coverage area\")\n",
    "            \n",
    "            # Print a sample of sediment geometries to compare\n",
    "            if not sedimentCoverage.empty:\n",
    "                sample_sediment = sedimentCoverage.iloc[0]\n",
    "                print(f\"Sample sediment bounds: {sample_sediment.geometry.bounds}\")\n",
    "            \n",
    "            return []\n",
    "    \n",
    "    # Check if 'tsym' column exists\n",
    "    if 'tsym' not in sediment_types.columns:\n",
    "        # Try to find a suitable column for soil types\n",
    "        print(f\"Available columns in sediment data: {sediment_types.columns.tolist()}\")\n",
    "        soil_type_columns = [col for col in sediment_types.columns if 'type' in col.lower() or 'sym' in col.lower() or 'soil' in col.lower()]\n",
    "        \n",
    "        if soil_type_columns:\n",
    "            soil_type_column = soil_type_columns[0]\n",
    "            print(f\"Using '{soil_type_column}' instead of 'tsym' for soil types\")\n",
    "            soil_types = sediment_types[soil_type_column].unique().tolist()\n",
    "        else:\n",
    "            print(f\"No suitable soil type column found\")\n",
    "            return []\n",
    "    else:\n",
    "        # Extract soil types from the filtered data\n",
    "        soil_types = sediment_types['tsym'].unique().tolist()\n",
    "    \n",
    "    print(f\"Found soil types: {soil_types}\")\n",
    "    return soil_types\n",
    "\n",
    "def gather_soil_types(purculation_mapping):\n",
    "    \"\"\"\n",
    "    Create a dictionary of soil types with their average percolation rates.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------    \n",
    "    purculation_mapping : dict\n",
    "        Dictionary with soil types as keys and min/max percolation rates\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with soil types as keys and average percolation rates as values\n",
    "    \n",
    "    Called by: load_process_data()\n",
    "    \"\"\"\n",
    "    # Take perculation Keys and the min and max / 2 and add to a dict\n",
    "    soil_types = {}\n",
    "    for key, value in purculation_mapping.items():\n",
    "        min = 0.0001 if value['min'] == 0 else value['min']\n",
    "        max = 0.9999 if value['max'] == 1 else value['max']\n",
    "            \n",
    "        soil_types[key] = (min + max) / 2\n",
    "    return soil_types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# SECTION 3: WATER CALCULATIONS           #\n",
    "###########################################\n",
    "\n",
    "def calculate_water_on_ground(df, soil_types, absorbtions, station):\n",
    "    \"\"\"\n",
    "    Calculate water on ground for specific soil types and station.\n",
    "\n",
    "    Parameters:\n",
    "    -----------    \n",
    "    df : pandas.DataFrame\n",
    "        Dataframe containing precipitation data with a 'Nedbor' column\n",
    "    soil_types : list\n",
    "        List of soil types to calculate water on ground for\n",
    "    absorbtions : dict\n",
    "        Dictionary with soil types and their absorption rates\n",
    "    station : str\n",
    "        Station ID to use in column naming\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame: Dataframe with water on ground values for the specified soil types\n",
    "    \n",
    "    Called by: load_process_data()\n",
    "    \"\"\"\n",
    "    # Get precipitation values as numpy array for faster calculations\n",
    "    precip_array = df['Nedbor'].values\n",
    "    n = len(precip_array)\n",
    "    \n",
    "    # Process all soil types at once using numpy operations\n",
    "    soil_type_data = {}\n",
    "    new_columns = {}  # Dict to collect all columns before creating DataFrame\n",
    "    valid_soil_types = [st for st in soil_types if st in absorbtions]\n",
    "    \n",
    "    if not valid_soil_types:\n",
    "        print(f\"No valid soil types with known absorption rates for station {station}\")\n",
    "        return df.copy()\n",
    "        \n",
    "    # Pre-allocate numpy arrays for all calculations to avoid memory allocations in loops\n",
    "    for soil_type in valid_soil_types:\n",
    "        rate = absorbtions[soil_type] \n",
    "        soil_type_data[soil_type] = {\n",
    "            'rate': rate,\n",
    "            'wog_array': np.zeros(n),\n",
    "            'observed': np.zeros(n, dtype=int),\n",
    "            'tte': np.full(n, n),  # Fill with max value initially\n",
    "            'duration': np.zeros(n, dtype=int)\n",
    "        }\n",
    "    \n",
    "    # Parallel WOG calculation for each soil type using vectorized operations where possible\n",
    "    for soil_type, data in soil_type_data.items():\n",
    "        rate = data['rate']\n",
    "        wog = data['wog_array']\n",
    "        \n",
    "        # First time step\n",
    "        wog[0] = max(0, precip_array[0])\n",
    "        \n",
    "        # Vectorized recurrence relation using a cumulative approach\n",
    "        for i in range(1, n):\n",
    "            wog[i] = max(0, wog[i-1] * (1 - rate) + precip_array[i])\n",
    "        \n",
    "        # Calculate observed state (> threshold)\n",
    "        # wog_window = np.convolve(wog, np.ones(3)/3, mode='same')  # 3-hour window\n",
    "        # data['observed'] = (wog_window > 5).astype(int)\n",
    "\n",
    "        data['observed'] = (wog > 5).astype(int) # CHANGE HERE!\n",
    "        \n",
    "        # Find event indices\n",
    "        event_indices = np.where(data['observed'] == 1)[0]\n",
    "        # First pass: Calculate time until next event (survival analysis approach)\n",
    "        tte = np.full(n, n)  # Default to maximum for censored observations\n",
    "        durations = np.full(n, n)  # Default to maximum\n",
    "\n",
    "        # Mark events with time-to-event = 0\n",
    "        tte[event_indices] = 0\n",
    "\n",
    "        # For each pair of events, calculate time between them\n",
    "        for i in range(len(event_indices)-1):\n",
    "            start_idx = event_indices[i]\n",
    "            end_idx = event_indices[i+1]\n",
    "            time_between = end_idx - start_idx\n",
    "            \n",
    "            # Fill in counting up from 1 at non-event to event time at event\n",
    "            for j in range(start_idx+1, end_idx):\n",
    "                tte[j] = end_idx - j\n",
    "            \n",
    "            # Store the duration (time until next event)\n",
    "            durations[start_idx:end_idx] = np.arange(1, time_between+1)\n",
    "\n",
    "        # For observations after the last event, they're all censored\n",
    "        if len(event_indices) > 0:\n",
    "            last_event = event_indices[-1]\n",
    "            durations[last_event+1:] = np.arange(1, n-last_event)\n",
    "\n",
    "        # Store calculated values\n",
    "        data['tte'] = tte\n",
    "        data['duration'] = durations\n",
    "            \n",
    "        # Add columns to the dictionary\n",
    "        new_columns[f'{station}_WOG_{soil_type}'] = data['wog_array']\n",
    "        new_columns[f\"{station}_{soil_type}_observed\"] = data['observed']\n",
    "        new_columns[f'{station}_{soil_type}_TTE'] = data['tte']\n",
    "        new_columns[f'{station}_{soil_type}_duration'] = data['duration']\n",
    "    \n",
    "    # Create new DataFrame with all columns at once\n",
    "    new_df = pd.DataFrame(new_columns, index=df.index)\n",
    "    \n",
    "    # Combine with original data\n",
    "    result_df = pd.concat([df, new_df], axis=1)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# SECTION 4: DATA LOADING AND SAVING      #\n",
    "###########################################\n",
    "\n",
    "def load_process_data(coverage_data=None, sediment_data=None):\n",
    "    \"\"\"\n",
    "    Load precipitation data and calculate water-on-ground values for different soil types.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    coverage_data : GeoDataFrame, optional\n",
    "        Pre-loaded precipitation coverage data\n",
    "    sediment_data : GeoDataFrame, optional\n",
    "        Pre-loaded sediment coverage data\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame: Processed data with soil type observations and durations\n",
    "    \n",
    "    Called by: main\n",
    "    Calls: gather_soil_types(), sediment_types_for_station(), calculate_water_on_ground(), save_preprocessed_data()\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the data\n",
    "        print(\"\\nLoading precipitation imputed data...\")\n",
    "        df = pd.read_parquet(\"data/raw/precipitation_imputed_data.parquet\")\n",
    "        # print(f\"Loaded precipitation data with columns: {df.columns.tolist()[:]}...\")\n",
    "        print(f\"This is a total number of columns: {len(df.columns)}, which is the number of stations\")\n",
    "        # each row is an hour of precipitation data for each station\n",
    "        # save as csv for easier debugging\n",
    "        df.to_csv(\"data/raw/precipitation_imputed_data.csv\", index=False)\n",
    "        print(f\"Precipitation data shape: {df.shape}\") #(262783, 86)\n",
    "\n",
    "        # total number of nans across all columns (all stations)\n",
    "        print(f\"Total number of NaNs in the data: {df.isna().sum().sum()}\")\n",
    "\n",
    "        # look column-wise for the number of nans\n",
    "        stations_with_most_nans = df.isna().sum().sort_values(ascending=False)\n",
    "        print(f\"Top 5 Stations with the most NaNs in procent of that station: {(stations_with_most_nans / len(df) * 100).head(5)}\")\n",
    "        # here we can divide by len(df) because all columns have the same length\n",
    "\n",
    "        # before clipping to remove extreme values, we need to check the data\n",
    "        # Check for extreme values in the data\n",
    "        # print(f\"Precipitation data summary:\\n{df.describe()}\")\n",
    "\n",
    "        # def length before clipping - which just replaces values lower than 0 with 0 and values higher than 60 with 60\n",
    "        # clip does not replace Nans, only limits existing values \n",
    "        df = df.clip(lower=0, upper=100) \n",
    "        # check min and max values\n",
    "        # The highest are the 2011 and 2014 cloudbursts, with 2014 possibly peaking around 119 mm. Official records might show around 115 mm.\n",
    "\n",
    "        #https://international.kk.dk/sites/default/files/2021-09/Cloudburst%20Management%20plan%202010.pdf?utm_source=chatgpt.com\n",
    "        # precipitation measured close to 100 mm in one hour.\n",
    "\n",
    "        #https://web.archive.org/web/20140913151609/http://vejret.tv2.dk/artikel/id-32909558:et-af-de-kraftigste-regnvejr-nogensinde.html\n",
    "        # over 100mm in 24 hours and private measurements for 160mm in 124 hours\n",
    "\n",
    "        #https://ui.adsabs.harvard.edu/abs/2021AGUFMGC45G0892C/abstract\n",
    "        #Between 90 and 135 mm of precipitation in less than 2 hours was recorded\n",
    "\n",
    "        #https://vejr.tv2.dk/2019-12-28-her-er-de-danske-vejrrekorder-fra-de-seneste-10-aar\n",
    "        # Here the record is 63mm in 30mins\n",
    "\n",
    "        #https://vejr.tv2.dk/2016-07-02-husker-du-vejret-den-2-juli-2011-historisk-skybrud-ramte-koebenhavn\n",
    "        # Kraftig regn er, nr der falder mere end 24 millimeter regn over en periode p maksimalt seks timer.\n",
    "        # Skybrud er, nr der falder mere end 15 millimeter regn over en periode p maksimalt 30 minutter.\n",
    "        # def length after clipping\n",
    "\n",
    "        # Use provided coverage data or load from file\n",
    "        if coverage_data is not None:\n",
    "            precipitationCoverageStations = coverage_data\n",
    "            print(f\"Using provided precipitation coverage with {len(precipitationCoverageStations)} stations\")\n",
    "        else:\n",
    "            print(\"Loading precipitation coverage stations...\")\n",
    "            precipitationCoverageStations = gu.load_geojson(\"precipitation_coverage.geojson\")\n",
    "            print(f\"Loaded precipitation coverage with {len(precipitationCoverageStations)} stations\")\n",
    "        \n",
    "        # Use provided sediment data or load from file\n",
    "        if sediment_data is not None:\n",
    "            sedimentCoverage = sediment_data\n",
    "            print(f\"Using provided sediment coverage with {len(sedimentCoverage)} features\")\n",
    "        else:\n",
    "            print(\"\\nLoading sediment coverage...\")\n",
    "            sedimentCoverage = gu.load_geojson(\"Sediment_wgs84.geojson\")\n",
    "            print(f\"Loaded sediment coverage with {len(sedimentCoverage)} features\")\n",
    "\n",
    "        # Get absorption rates for each soil type\n",
    "        absorbtions = gather_soil_types(pm.percolation_rates_updated)\n",
    "        print(f\"Gathered absorption rates for {len(absorbtions)} soil types\")\n",
    "        \n",
    "        print(f\"\\nPrecipitaion dataset columns: {df.columns.tolist()}\")\n",
    "        stations_to_process = df.columns\n",
    "\n",
    "        # For each station in the data, calculate the water on ground for each soil type\n",
    "        for station in stations_to_process:\n",
    "            print(f\"Processing station {station}...\")\n",
    "            df_station = df[[station]].copy() # this is a single column dataframe with the precipitation data for this station\n",
    "            \n",
    "            # Rename the station name column to 'Nedbor' (precipitation) for consistency\n",
    "            df_station.rename(columns={station: 'Nedbor'}, inplace=True)\n",
    "\n",
    "            # length before dropping NaN values\n",
    "            pre_drop_nans = len(df_station)\n",
    "            print(f\"   Precipitation data {station} length before dropping NaN: {len(df_station)}\")\n",
    "            df_station.dropna(inplace=True) \n",
    "            # we remove the rows with NaN values, because they are not useful for our calculations \n",
    "            # as we want to calculate the water on ground only for the rows with precipitation data\n",
    "            print(f\"   Removed {pre_drop_nans - len(df_station)} NaN values (procent {(pre_drop_nans - len(df_station)) / pre_drop_nans * 100:.2f}%)\")\n",
    "            \n",
    "            if df_station.empty:\n",
    "                print(f\"No data for station {station}, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            # Get soil types for this station\n",
    "            sediment_types = sediment_types_for_station(station, precipitationCoverageStations, sedimentCoverage)\n",
    "            \n",
    "            if not sediment_types:\n",
    "                print(f\"No sediment types found for station {station}, skipping...\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"Found {len(sediment_types)} sediment types for station {station}\")\n",
    "            \n",
    "            # Calculate water on ground for each soil type\n",
    "            try:\n",
    "                df_processed = calculate_water_on_ground(df_station, sediment_types, absorbtions, station)\n",
    "                # Add processed columns to results (excluding 'Nedbor')\n",
    "                result_columns = df_processed.drop(columns=['Nedbor'], errors='ignore')\n",
    "                if not result_columns.empty:\n",
    "                    save_preprocessed_data(result_columns, f\"data/processed/survival_data_{station}.csv\")\n",
    "                    print(f\"Saved processed data for station {station}\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR processing station {station}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Combine all result DataFrames at once\n",
    "        return None  # Return None to indicate completion\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR in load_process_data: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()  # Return empty DataFrame on error\n",
    "\n",
    "def save_preprocessed_data(survival_df, output_path=\"data/processed/survival_data.csv\"):\n",
    "    \"\"\"\n",
    "    Save the processed survival data to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------    \n",
    "    survival_df : DataFrame\n",
    "        DataFrame containing survival data for different soil types and stations\n",
    "    output_path : str\n",
    "        Path to save the combined CSV file\n",
    "    \n",
    "    Called by: load_process_data()\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    # Save to Parquet for better performance\n",
    "    try:\n",
    "        survival_df.to_parquet(output_path.replace('.csv', '.parquet'), index=False)\n",
    "        print(f\"Saved preprocessed survival data to {output_path.replace('.csv', '.parquet')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR saving to Parquet: {e}\")\n",
    "        print(\"Falling back to CSV format\")\n",
    "        try:\n",
    "            survival_df.to_csv(output_path, index=False)\n",
    "            print(f\"Saved preprocessed survival data to {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR saving to CSV: {e}\")\n",
    "            print(\"Failed to save preprocessed data\")\n",
    "            return None\n",
    "    \n",
    "    return survival_df\n",
    "\n",
    "def load_saved_data(file_path=\"data/processed/survival_data.csv\"):\n",
    "    \"\"\"\n",
    "    Load previously saved preprocessed data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------    \n",
    "    file_path : str\n",
    "        Path to the saved data file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with soil types as keys and survival dataframes as values\n",
    "    \n",
    "    Called by: main\n",
    "    \"\"\"\n",
    "\n",
    "    import os\n",
    "\n",
    "    # load parquet file if it exists\n",
    "    try:\n",
    "        if os.path.exists(file_path.replace('.csv', '.parquet')):\n",
    "            survival_df = pd.read_parquet(file_path.replace('.csv', '.parquet'))\n",
    "            print(f\"Loaded preprocessed data from {file_path.replace('.csv', '.parquet')}\")\n",
    "            return survival_df\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading Parquet data: {e}\")\n",
    "\n",
    "    try:\n",
    "        # Load the combined dataframe\n",
    "        survival_df = pd.read_csv(file_path)\n",
    "        \n",
    "        return survival_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading data from {file_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###########################################\n",
    "# MAIN EXECUTION                          #\n",
    "###########################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # TODO uncomment this\n",
    "    # Step 1: Create coverage areas for precipitation stations\n",
    "    coverage_geojson_gdf, stations_gdf = create_full_coverage()\n",
    "    # this functions saves the coverage_geojson_gdf to a file called precipitation_coverage.geojson\n",
    "    if coverage_geojson_gdf is None:\n",
    "        print(\"ERROR No valid coverage data created. Attemption to load from file.\")\n",
    "        try:\n",
    "            coverage_geojson_gdf = gu.load_geojson(\"precipitation_coverage.geojson\")\n",
    "            print(\"Loaded coverage data from file.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR loading coverage data from file: {e}\")\n",
    "            print(\"Exiting.\")\n",
    "            exit(1)\n",
    "    \n",
    "    # Step 2: Load sediment data - this was the layer that was exported from the QGIS project\n",
    "    print(\"Loading Sediment_wgs84.geojson...\")\n",
    "    sedimentCoverage = gu.load_geojson(\"Sediment_wgs84.geojson\")\n",
    "    if sedimentCoverage is None:\n",
    "        print(\"No valid sediment data loaded. Exiting.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Step 3: Process precipitation and soil data\n",
    "    # - this gathers the soil types\n",
    "    # For each station, it calculates\n",
    "    # * sediment types for the given station\n",
    "    # * water on ground for each soil type\n",
    "    # * saves the processed data to a CSV file\n",
    "    load_process_data(coverage_data=coverage_geojson_gdf, sediment_data=sedimentCoverage)\n",
    "\n",
    "    # Step 4: TESTING - Load and display sample processed data for a specific station\n",
    "    station_id = '06058'  # Example station ID\n",
    "    df = load_saved_data(f'data/processed/survival_data_{station_id}.csv')\n",
    "    if df is not None:\n",
    "        print(f\"\\nSample data for station {station_id}:\")\n",
    "        print(df.head())\n",
    "    else:\n",
    "        print(f\"No data found for station {station_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling predictions of extreme events ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Forecasting ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate predictions for future precipitation events, we need a model capable of forecasting precipitation based on historical data. Initially, we planned to use a traditional time series forecasting model. This approach can be extended to include deep learning methods by leveraging pre-built forecasting frameworks, which are easy to use and provide strong results without requiring extensive manual tuning of parameterized models.\n",
    "\n",
    "After reviewing available tools, we chose Metas NeuralProphet. NeuralProphet is a hybrid forecasting model that combines deep learning techniques with classical time series components, such as ARIMA-like autoregression and exponential smoothing (ETS). The framework is built on PyTorch, is highly scalable, and is user-friendly. In our implementation, we apply NeuralProphet to our dataset, using monthly accumulated precipitation as the forecast target. This transformation helps illustrate both the models capabilities and its limitations when applied to our specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if station not in df.columns:\n",
    "    raise ValueError(\"Expected column station not found.\")\n",
    "\n",
    "df[station] = df[station].clip(lower=0)\n",
    "df = df[[station]]\n",
    "df = df.reset_index()\n",
    "df['date'] = df['index'].dt.date\n",
    "df = df.rename(columns={station: 'precipitation'})\n",
    "df = df.drop(columns='index')\n",
    "df = df.dropna(subset=['precipitation']).reset_index(drop=True)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['year_month'] = df['date'].dt.to_period('M')\n",
    "df_monthly = df.groupby('year_month')['precipitation'].sum().reset_index()\n",
    "df_monthly['year_month'] = df_monthly['year_month'].dt.to_timestamp()\n",
    "df_prophet = df_monthly.rename(columns={'year_month': 'ds', 'precipitation': 'y'})\n",
    "\n",
    "# --- Quick data exploration ---\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(df_prophet['ds'], df_prophet['y'], marker='o')\n",
    "# plt.title('Monthly Precipitation Over Time')\n",
    "# plt.xlabel('Date')\n",
    "# plt.ylabel('Precipitation (mm)')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# --- Train-test split for evaluation ---\n",
    "split_idx = int(len(df_prophet) * 0.8)\n",
    "df_train = df_prophet.iloc[:split_idx]\n",
    "df_test = df_prophet.iloc[split_idx:]\n",
    "\n",
    "# --- Initialize NeuralProphet model ---\n",
    "m = NeuralProphet(\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=False,\n",
    "    daily_seasonality=False,\n",
    "    #seasonality_mode='multiplicative',\n",
    "    n_changepoints=20,\n",
    "    changepoints_range=0.9,\n",
    "    trend_reg=1,\n",
    "    quantiles=[0.1, 0.9]\n",
    ")\n",
    "\n",
    "m = m.add_seasonality(name='monthly', period=30.5, fourier_order=5)\n",
    "\n",
    "# --- Fit model on training set ---\n",
    "metrics = m.fit(df_train, freq='M')\n",
    "\n",
    "# --- Forecast on train + test periods ---\n",
    "future = m.make_future_dataframe(df_train, periods=len(df_test), n_historic_predictions=True)\n",
    "forecast = m.predict(future)\n",
    "\n",
    "# # --- Plot forecast including quantiles ---\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(forecast['ds'], forecast['yhat1'], label='Prediction (Median)', color='blue')\n",
    "# if 'yhat1_lower' in forecast.columns and 'yhat1_upper' in forecast.columns:\n",
    "#     plt.fill_between(forecast['ds'], forecast['yhat1_lower'], forecast['yhat1_upper'], color='blue', alpha=0.3, label='80% Prediction Interval')\n",
    "# plt.scatter(df_prophet['ds'], df_prophet['y'], color='black', s=10, label='Actual Data')\n",
    "# plt.title('Monthly Precipitation Forecast with Uncertainty Interval')\n",
    "# plt.xlabel('Date')\n",
    "# plt.ylabel('Precipitation (mm)')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# --- Plot model components (trend, seasonality, etc.) ---\n",
    "fig_components = m.plot_components(forecast)\n",
    "\n",
    "# --- Plot model parameters ---\n",
    "fig_parameters = m.plot_parameters()\n",
    "\n",
    "# --- Evaluation on test set ---\n",
    "forecast_test = forecast.iloc[-len(df_test):]\n",
    "mae = mean_absolute_error(df_test['y'].values, forecast_test['yhat1'].values)\n",
    "r2 = r2_score(df_test['y'].values, forecast_test['yhat1'].values)\n",
    "print(f\"Test MAE: {mae:.2f}\")\n",
    "print(f\"Test R: {r2:.2f}\")\n",
    "\n",
    "# # --- Plot actual vs predicted for test period ---\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(df_test['ds'], df_test['y'], label='Actual', marker='o')\n",
    "# plt.plot(df_test['ds'], forecast_test['yhat1'], label='Predicted', marker='x')\n",
    "# if 'yhat1_lower' in forecast_test.columns and 'yhat1_upper' in forecast_test.columns:\n",
    "#     plt.fill_between(df_test['ds'], forecast_test['yhat1_lower'], forecast_test['yhat1_upper'], color='blue', alpha=0.2, label='80% Interval')\n",
    "# plt.title('Actual vs Predicted Precipitation (Test Set with Uncertainty)')\n",
    "# plt.xlabel('Date')\n",
    "# plt.ylabel('Precipitation (mm)')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# --- Print final few metrics ---\n",
    "print(metrics.tail())\n",
    "# --- Forecast next 48 months into the future ---\n",
    "future_48 = m.make_future_dataframe(df_prophet, periods=48, n_historic_predictions=True)  # <--- notice: True\n",
    "forecast_48 = m.predict(future_48)\n",
    "forecast_48['yhat1'] = forecast_48['yhat1'].clip(lower=0)\n",
    "if 'yhat1_lower' in forecast_48.columns:\n",
    "    forecast_48['yhat1_lower'] = forecast_48['yhat1_lower'].clip(lower=0)\n",
    "    forecast_48['yhat1_upper'] = forecast_48['yhat1_upper'].clip(lower=0)\n",
    "\n",
    "\n",
    "# --- Plot historical + forecasted precipitation together ---\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot historical observed data\n",
    "plt.scatter(df_prophet['ds'], df_prophet['y'], color='black', s=10, label='Actual Data')\n",
    "\n",
    "# Plot fitted values (up to today)\n",
    "plt.plot(forecast_48['ds'], forecast_48['yhat1'], label='Forecast (Median)', color='blue')\n",
    "\n",
    "# Plot uncertainty if available\n",
    "if 'yhat1_lower' in forecast_48.columns and 'yhat1_upper' in forecast_48.columns:\n",
    "    plt.fill_between(forecast_48['ds'], forecast_48['yhat1_lower'], forecast_48['yhat1_upper'], color='blue', alpha=0.3, label='80% Prediction Interval')\n",
    "\n",
    "plt.title('Historical and 48-Month Future Forecast of Precipitation')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Precipitation (mm)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The limitations of this approach become immediately clear. While NeuralProphet provides a solid, smoothed forecast based on historical data, our objective requires a model that can respond to extreme precipitation events, such as those that could lead to flooding. NeuralProphet, and time series modelling in general, are not well suited to predict these outliers. These events are both rare and seemingly random, and time series models tend to optimize for overall accuracy, not the extremes.\n",
    "\n",
    " To make a time series forcasting model useful, we would essentially have to extend the model with a term, that would require the model to predict rare events. This would however, not be based on realistic predictions, unless external variables were added to this term. An option could be a multivariate model forecasting model, incorporating variables such as temperature, humidity, wind etc. However, the data collection process would increase the complexity and workload of our project considerably, and was therefore deemed unrealistic. \n",
    "\n",
    "As a result of this process, we switched methods, and opted to incorporate 'Survival Analysis', to predict the probability of rare events occuring in a given timeframe. This method is specifically designed to predict these rare events, and are possible to build using the percipiation data available to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survival Analysis ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurvivalModel:\n",
    "    def __init__(self, soil_type='clay'):\n",
    "        self.model = ExponentialFitter()\n",
    "        self.soil_type = soil_type\n",
    "        self.station = None  # Placeholder for station data\n",
    "        self.is_fitted = False\n",
    "        self.units = 'hours'  # Default unit for duration \n",
    "    \n",
    "    def train(self, df, duration_column='duration', event_column='observed'):\n",
    "        \"\"\"Train the survival model on dry spell durations.\"\"\"\n",
    "        if df is None or len(df) == 0:\n",
    "            raise ValueError(\"Training data is empty\")\n",
    "            \n",
    "        self.model.fit(durations=df[duration_column], event_observed=df[event_column])\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, durations):\n",
    "        \"\"\"\n",
    "        Predict probability of rain occurring by the given duration.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        durations : int, array-like\n",
    "            Number of time units to predict probability for\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        array-like : Probability of rain occurring by the specified duration\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Model must be trained before making predictions\")\n",
    "            \n",
    "        # Convert to array if single value\n",
    "        if isinstance(durations, (int, float)):\n",
    "            durations = [durations]\n",
    "            \n",
    "        # Get survival function (probability of remaining dry)\n",
    "        survival_probs = self.model.predict(durations)\n",
    "        \n",
    "        # Return probability of rain (1 - survival probability)\n",
    "        return 1 - survival_probs\n",
    "    \n",
    "    def predict(self, year):\n",
    "        \"\"\"\n",
    "        Predict the probability of rain occurring before a given year (int)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        year : int\n",
    "            Number of years into the future\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float : Probability of rain occurring by the specified year\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Model must be trained before making predictions\")\n",
    "        \n",
    "        # Get the maximum observed duration from the model\n",
    "        max_observed = self.model.timeline.max()\n",
    "        \n",
    "        # Calculate duration based on the year\n",
    "        if self.units == 'hours':\n",
    "            # Scale down for better distribution across years\n",
    "            # This prevents all predictions from being 100%\n",
    "            if year <= 0.1:  # For very short periods\n",
    "                duration = year * 365 * 24 * 0.1  # Scale down for short durations\n",
    "            elif year <= 1:  # For periods up to a year\n",
    "                duration = year * 365 * 24 * 0.2  # Scale down a bit\n",
    "            else:  # For longer periods\n",
    "                # Use a logarithmic scale to prevent saturation at 100%\n",
    "                duration = min(max_observed * (1 + np.log(year)), max_observed)\n",
    "        elif self.units == 'days':\n",
    "            if year <= 1:\n",
    "                duration = year * 365 * 0.5\n",
    "            else:\n",
    "                duration = min(max_observed * (1 + np.log(year)), max_observed)\n",
    "        else:  # years or other units\n",
    "            duration = min(year, max_observed)\n",
    "            \n",
    "        #print(f\"Soil type: {self.soil_type}, Year: {year}, Duration: {duration}, Max observed: {max_observed}\")\n",
    "        \n",
    "        # Get probability and apply a dampening function to avoid 100% predictions\n",
    "        # as years increase\n",
    "        raw_prob = float(self.predict_proba(duration))\n",
    "        \n",
    "        # Apply a dampening function for multi-year predictions\n",
    "        if year > 1:\n",
    "            # Dampened probability that approaches but never quite reaches 100%\n",
    "            prob = raw_prob * (1 - 0.1 / year)\n",
    "            #print(f\"Year {year}: Raw prob {raw_prob:.4f}, dampened to {prob:.4f}\")\n",
    "        else:\n",
    "            prob = raw_prob\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def plot(self):\n",
    "        \"\"\"Plot the survival function.\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Model must be trained before plotting\")\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        self.model.plot_cumulative_density()\n",
    "        plt.title(f\"Survival Function for {self.soil_type}\")\n",
    "        plt.xlabel(\"Duration (hours)\")\n",
    "        plt.ylabel(\"Survival Probability\")\n",
    "        plt.grid()\n",
    "        plt.savefig(f\"{self.soil_type}_survival_function.png\")\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Save the fitted Kaplan-Meier model to disk.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        path : str\n",
    "            File path where the model should be saved\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        self : SurvivalModel\n",
    "            Returns self for method chaining\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Model must be trained before saving\")\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        \n",
    "        # Save model and metadata\n",
    "        model_data = {\n",
    "            'model': self.model,\n",
    "            'soil_type': self.soil_type,\n",
    "            'station': self.station,\n",
    "            'units': self.units,\n",
    "            'is_fitted': self.is_fitted\n",
    "        }\n",
    "        pd.to_pickle(model_data, path)\n",
    "        return self\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"\n",
    "        Load a fitted Kaplan-Meier model from disk.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        path : str\n",
    "            File path to the saved model\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : SurvivalModel\n",
    "            Returns self with loaded model\n",
    "        \"\"\"\n",
    "        # Load model and metadata\n",
    "        model_data = pd.read_pickle(path)\n",
    "        \n",
    "        # Restore model attributes\n",
    "        self.model = model_data['model']\n",
    "        self.soil_type = model_data['soil_type']\n",
    "        self.station = model_data['station']\n",
    "        self.is_fitted = model_data['is_fitted']\n",
    "        self.units = model_data['units']\n",
    "        \n",
    "        return self\n",
    "\n",
    "class FloodModel:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.is_fitted = False\n",
    "        self.units = 'hours'\n",
    "        self.soil_types = [\"DG - Meltwater gravel\", \"DS - Meltwater sand\"]\n",
    "        self.stations = []\n",
    "        self.available_soil_types = []\n",
    "    \n",
    "    def add_station(self, station, survival_df, soiltypes):\n",
    "        \"\"\"\n",
    "        Add station data to the flood model and train survival models for each soil type.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        station : str\n",
    "            Station identifier\n",
    "        survival_df : pandas.DataFrame\n",
    "            DataFrame containing survival data for the station\n",
    "        soiltypes : list\n",
    "            List of soil types to train models for this station\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : FloodModel\n",
    "            Returns self for method chaining\n",
    "        \"\"\"\n",
    "        if station not in self.stations:\n",
    "            self.stations.append(station)\n",
    "            \n",
    "        # Create models for each soil type in this station\n",
    "        for soil_type in soiltypes:\n",
    "            # Create column names based on pattern in the dataframe\n",
    "            duration_column = f\"{station}_{soil_type}_duration\"\n",
    "            event_column = f\"{station}_{soil_type}_observed\"\n",
    "            \n",
    "            # Check if needed columns exist\n",
    "            if duration_column in survival_df.columns and event_column in survival_df.columns:\n",
    "                # Filter out any missing values\n",
    "                valid_data = survival_df[[duration_column, event_column]].dropna()\n",
    "                \n",
    "                if len(valid_data) > 0:\n",
    "                    # Create a model for this station-soil combination\n",
    "                    model_key = f\"{station}_{soil_type}\"\n",
    "                    \n",
    "                    # Create and train the model\n",
    "                    model = SurvivalModel(soil_type=soil_type)\n",
    "                    model.station = station\n",
    "                    model.train(\n",
    "                        valid_data.rename(columns={\n",
    "                            duration_column: 'duration',\n",
    "                            event_column: 'observed'\n",
    "                        }),\n",
    "                        'duration', \n",
    "                        'observed'\n",
    "                    )\n",
    "                    \n",
    "                    # Add to our models dictionary\n",
    "                    self.models[model_key] = model\n",
    "                    \n",
    "                    # Add to available soil types if not already there\n",
    "                    if soil_type not in self.available_soil_types:\n",
    "                        self.available_soil_types.append(soil_type)\n",
    "                    \n",
    "                    print(f\"Trained model for station {station}, soil type {soil_type} with {len(valid_data)} observations\")\n",
    "                else:\n",
    "                    print(f\"No valid data for station {station}, soil type {soil_type}\")\n",
    "            else:\n",
    "                print(f\"Missing columns for station {station}, soil type {soil_type}\")\n",
    "        \n",
    "        # Mark as fitted if we have any models\n",
    "        if self.models:\n",
    "            self.is_fitted = True\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def save(self, path, split_by_station=True):\n",
    "        \"\"\"\n",
    "        Save the FloodModel to disk.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        path : str\n",
    "            File path where the model should be saved\n",
    "        split_by_station : bool\n",
    "            If True, save each station's models in a separate file\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : FloodModel\n",
    "            Returns self for method chaining\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Model must be trained before saving\")\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        \n",
    "        # Split storage by station\n",
    "        if split_by_station and len(self.models) > 0:\n",
    "            # Extract base directory and filename without extension\n",
    "            base_dir = os.path.dirname(path)\n",
    "            base_name = os.path.splitext(os.path.basename(path))[0]\n",
    "            \n",
    "            # Create stations directory\n",
    "            stations_dir = os.path.join(base_dir, f\"{base_name}_stations\")\n",
    "            os.makedirs(stations_dir, exist_ok=True)\n",
    "            \n",
    "            print(f\"Starting split save: {len(self.stations)} stations with {len(self.models)} models...\")\n",
    "            \n",
    "            # Group models by station\n",
    "            station_models = {}\n",
    "            for model_key, model in self.models.items():\n",
    "                station = model_key.split('_')[0]\n",
    "                if station not in station_models:\n",
    "                    station_models[station] = {}\n",
    "                station_models[station][model_key] = model\n",
    "            \n",
    "            # Create a metadata model that references station files\n",
    "            meta_model = {\n",
    "                'is_fitted': True,\n",
    "                'units': self.units,\n",
    "                'soil_types': self.soil_types,\n",
    "                'stations': self.stations,\n",
    "                'available_soil_types': self.available_soil_types,\n",
    "                'station_paths': {}  # Will store paths to station model files\n",
    "            }\n",
    "            \n",
    "            # Save each station separately\n",
    "            saved_files = 0\n",
    "            for station, models in station_models.items():\n",
    "                station_path = os.path.join(stations_dir, f\"station_{station}.joblib\")\n",
    "                \n",
    "                # Display progress every 10 stations\n",
    "                if saved_files % 10 == 0:\n",
    "                    print(f\"Saving station {saved_files}/{len(station_models)}: {station} with {len(models)} models...\")\n",
    "                    \n",
    "                # Save station models with joblib\n",
    "                joblib.dump(models, station_path, compress=3)\n",
    "                \n",
    "                # Store the relative path in metadata\n",
    "                meta_model['station_paths'][station] = os.path.relpath(station_path, base_dir)\n",
    "                saved_files += 1\n",
    "                \n",
    "            # Save the metadata file\n",
    "            print(f\"Saving metadata to {path}...\")\n",
    "            joblib.dump(meta_model, path, compress=3)\n",
    "            print(f\"Successfully saved {saved_files} station files and metadata\")\n",
    "            \n",
    "        else:\n",
    "            # Traditional single-file save\n",
    "            print(f\"Starting to save {len(self.models)} models to {path}...\")\n",
    "            model_data = {\n",
    "                'models': self.models,\n",
    "                'is_fitted': self.is_fitted,\n",
    "                'units': self.units,\n",
    "                'soil_types': self.soil_types,\n",
    "                'stations': self.stations,\n",
    "                'available_soil_types': self.available_soil_types\n",
    "            }\n",
    "            joblib.dump(model_data, path, compress=3)\n",
    "            print(f\"Successfully saved model to {path}\")\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def load(self, path, lazy_load=True):\n",
    "        \"\"\"\n",
    "        Load a saved FloodModel from disk.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        path : str\n",
    "            File path to the saved model\n",
    "        lazy_load : bool\n",
    "            If True and model was saved with split_by_station=True, \n",
    "            only load station models when requested\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : FloodModel\n",
    "            Returns self with loaded models\n",
    "        \"\"\"\n",
    "        print(f\"Loading model from {path}...\")\n",
    "        \n",
    "        # Try to load model data\n",
    "        model_data = joblib.load(path)\n",
    "        \n",
    "        # Check if this is a split model (metadata file)\n",
    "        if isinstance(model_data, dict) and 'station_paths' in model_data:\n",
    "            # This is a split model - load metadata\n",
    "            self.is_fitted = model_data.get('is_fitted', False)\n",
    "            self.units = model_data.get('units', 'hours')\n",
    "            self.soil_types = model_data.get('soil_types', [])\n",
    "            self.stations = model_data.get('stations', [])\n",
    "            self.available_soil_types = model_data.get('available_soil_types', [])\n",
    "            \n",
    "            # Get base directory for relative paths\n",
    "            base_dir = os.path.dirname(path)\n",
    "            \n",
    "            if lazy_load:\n",
    "                # Create a proxy function for each station that will load data when needed\n",
    "                self.models = {}\n",
    "                print(f\"Lazy-loading enabled: Referenced {len(model_data['station_paths'])} stations\")\n",
    "                \n",
    "                # Store the station paths for later loading\n",
    "                self._station_paths = {\n",
    "                    station: os.path.join(base_dir, rel_path) \n",
    "                    for station, rel_path in model_data['station_paths'].items()\n",
    "                }\n",
    "            else:\n",
    "                # Load all station models immediately\n",
    "                self.models = {}\n",
    "                total_stations = len(model_data['station_paths'])\n",
    "                print(f\"Loading all {total_stations} station models...\")\n",
    "                \n",
    "                for i, (station, rel_path) in enumerate(model_data['station_paths'].items()):\n",
    "                    station_path = os.path.join(base_dir, rel_path)\n",
    "                    if i % 10 == 0:\n",
    "                        print(f\"Loading station {i+1}/{total_stations}: {station}...\")\n",
    "                    \n",
    "                    try:\n",
    "                        # Load the station models\n",
    "                        station_models = joblib.load(station_path)\n",
    "                        # Add to the main models dictionary\n",
    "                        self.models.update(station_models)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading station {station}: {e}\")\n",
    "        else:\n",
    "            # Traditional single-file model\n",
    "            self.models = model_data.get('models', {})\n",
    "            self.is_fitted = model_data.get('is_fitted', False)\n",
    "            self.units = model_data.get('units', 'hours')\n",
    "            self.soil_types = model_data.get('soil_types', [])\n",
    "            self.stations = model_data.get('stations', [])\n",
    "            self.available_soil_types = model_data.get('available_soil_types', [])\n",
    "        \n",
    "        print(f\"Model loaded with {len(self.stations)} stations\")\n",
    "        return self\n",
    "\n",
    "    def get_station_models(self, station):\n",
    "        \"\"\"\n",
    "        Get all models for a specific station.\n",
    "        Will load from disk if using lazy loading.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        station : str\n",
    "            Station identifier\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary of models for the station\n",
    "        \"\"\"\n",
    "        # Check if we're using lazy loading and need to load this station\n",
    "        if hasattr(self, '_station_paths') and station in self._station_paths:\n",
    "            # Station not loaded yet, load it now\n",
    "            station_path = self._station_paths[station]\n",
    "            print(f\"Loading station {station} models from {station_path}...\")\n",
    "            \n",
    "            try:\n",
    "                # Load the station models\n",
    "                station_models = joblib.load(station_path)\n",
    "                # Add to the main models dictionary\n",
    "                self.models.update(station_models)\n",
    "                # Return the loaded models for this station\n",
    "                return {k: v for k, v in station_models.items()}\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading station {station}: {e}\")\n",
    "                return {}\n",
    "        \n",
    "        # If not lazy loading or already loaded, filter existing models\n",
    "        return {k: v for k, v in self.models.items() if k.startswith(f\"{station}_\")}\n",
    "\n",
    "    def load_station(self, station, stations_dir):\n",
    "        \"\"\"\n",
    "        Load models for a specific station from the stations directory.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        station : str\n",
    "            Station identifier\n",
    "        stations_dir : str\n",
    "            Directory containing station model files\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary of loaded models for this station\n",
    "        \"\"\"\n",
    "        # Try different filename patterns\n",
    "        file_patterns = [\n",
    "            os.path.join(stations_dir, f\"{station}.joblib\"),\n",
    "            os.path.join(stations_dir, f\"station_{station}.joblib\")\n",
    "        ]\n",
    "        \n",
    "        loaded_models = {}\n",
    "        for file_path in file_patterns:\n",
    "            if os.path.exists(file_path):\n",
    "                try:\n",
    "                    print(f\"Loading station models from {file_path}\")\n",
    "                    station_models = joblib.load(file_path)\n",
    "                    \n",
    "                    # Add models to the main models dictionary\n",
    "                    if isinstance(station_models, dict):\n",
    "                        for model_key, model in station_models.items():\n",
    "                            self.models[model_key] = model\n",
    "                            loaded_models[model_key] = model\n",
    "                    \n",
    "                    # Return the loaded models\n",
    "                    return loaded_models\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading station file {file_path}: {e}\")\n",
    "        \n",
    "        print(f\"No valid model file found for station {station} in {stations_dir}\")\n",
    "        return {}\n",
    "    \n",
    "    def predict_proba(self, geodata, station_coverage, year):\n",
    "        \"\"\"\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Model must be trained before making predictions\")\n",
    "        \n",
    "        result_geodata = geodata.copy()\n",
    "        column_name = f'predictions_{year}'\n",
    "\n",
    "        soil_type_col = 'sediment'\n",
    "        if soil_type_col not in result_geodata.columns:\n",
    "            # Try to find a suitable column for soil types\n",
    "            possible_cols = [col for col in result_geodata.columns \n",
    "                        if 'soil' in col.lower() or 'type' in col.lower()]\n",
    "            if possible_cols:\n",
    "                soil_type_col = possible_cols[0]\n",
    "            else:\n",
    "                print(\"Could not find soil type column in geodata\")\n",
    "                return geodata\n",
    "         # Initialize prediction column\n",
    "        result_geodata[column_name] = None\n",
    "        \n",
    "        # If no station coverage provided, use all available stations with equal weight\n",
    "        # Spatial join to find which station coverage area each geometry falls into\n",
    "        # Ensure both GeoDataFrames have the same CRS\n",
    "        if result_geodata.crs != station_coverage.crs:\n",
    "            print(f\"Converting station_coverage from {station_coverage.crs} to {result_geodata.crs}\")\n",
    "            station_coverage = station_coverage.to_crs(result_geodata.crs)\n",
    "        \n",
    "        # Get the station ID column\n",
    "        station_id_col = 'station_id'\n",
    "        if station_id_col not in station_coverage.columns:\n",
    "            # Try to find a suitable station ID column\n",
    "            possible_cols = [col for col in station_coverage.columns \n",
    "                           if 'station' in col.lower() and 'id' in col.lower()]\n",
    "            if possible_cols:\n",
    "                station_id_col = possible_cols[0]\n",
    "            else:\n",
    "                print(\"Could not find station ID column in station_coverage\")\n",
    "                return geodata\n",
    "        \n",
    "        # Process each row in geodata\n",
    "        for idx, row in result_geodata.iterrows():\n",
    "            geometry = row.geometry\n",
    "            soil_type = row[soil_type_col]\n",
    "            \n",
    "            # Find which station coverage area this geometry intersects with\n",
    "            intersecting_stations = station_coverage[station_coverage.intersects(geometry)]\n",
    "            \n",
    "            if not intersecting_stations.empty:\n",
    "                # Extract first element if it's a compound soil type description\n",
    "                if isinstance(soil_type, str) and ' ' in soil_type:\n",
    "                    simple_type = soil_type.split(' ')[0]\n",
    "                else:\n",
    "                    simple_type = soil_type\n",
    "                    \n",
    "                # Get predictions from all intersecting stations and take the average\n",
    "                predictions = []\n",
    "                for _, station_row in intersecting_stations.iterrows():\n",
    "                    station = station_row[station_id_col]\n",
    "                    model_key = f\"{station}_{simple_type}\"\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    if model_key in self.models:\n",
    "                        try:\n",
    "                            model = self.models[model_key]\n",
    "                            predictions.append(model.predict(year))\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error predicting for {model_key}: {e}\")\n",
    "                \n",
    "                # Calculate the average prediction if we found any models\n",
    "                if predictions:\n",
    "                    result_geodata.at[idx, column_name] = sum(predictions) / len(predictions)\n",
    "                else:\n",
    "                    # No models found for this soil type at these stations, assign a default value\n",
    "                    default_value = min(0.2 + 0.05 * year, 0.0)\n",
    "                    result_geodata.at[idx, column_name] = default_value\n",
    "            else:\n",
    "                # Geometry doesn't intersect with any station coverage area\n",
    "                default_value = min(0.2 + 0.05 * year, 0.0)\n",
    "                result_geodata.at[idx, column_name] = default_value\n",
    "    \n",
    "        # Store raw probability values before percentage conversion\n",
    "        result_geodata[f'{column_name}_raw'] = result_geodata[column_name].copy()\n",
    "        \n",
    "        # Convert to percentage for visualization\n",
    "        result_geodata[column_name] = result_geodata[column_name] * 100\n",
    "        \n",
    "        return result_geodata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution of the model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FloodModel:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.is_fitted = False\n",
    "        self.units = 'hours'\n",
    "        self.soil_types = [\"DG - Meltwater gravel\", \"DS - Meltwater sand\"]\n",
    "        self.stations = []\n",
    "        self.available_soil_types = []\n",
    "    \n",
    "    def add_station(self, station, survival_df, soiltypes):\n",
    "        \"\"\"\n",
    "        Add station data to the flood model and train survival models for each soil type.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        station : str\n",
    "            Station identifier\n",
    "        survival_df : pandas.DataFrame\n",
    "            DataFrame containing survival data for the station\n",
    "        soiltypes : list\n",
    "            List of soil types to train models for this station\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : FloodModel\n",
    "            Returns self for method chaining\n",
    "        \"\"\"\n",
    "        if station not in self.stations:\n",
    "            self.stations.append(station)\n",
    "            \n",
    "        # Create models for each soil type in this station\n",
    "        for soil_type in soiltypes:\n",
    "            # Create column names based on pattern in the dataframe\n",
    "            duration_column = f\"{station}_{soil_type}_duration\"\n",
    "            event_column = f\"{station}_{soil_type}_observed\"\n",
    "            \n",
    "            # Check if needed columns exist\n",
    "            if duration_column in survival_df.columns and event_column in survival_df.columns:\n",
    "                # Filter out any missing values\n",
    "                valid_data = survival_df[[duration_column, event_column]].dropna()\n",
    "                \n",
    "                if len(valid_data) > 0:\n",
    "                    # Create a model for this station-soil combination\n",
    "                    model_key = f\"{station}_{soil_type}\"\n",
    "                    \n",
    "                    # Create and train the model\n",
    "                    model = SurvivalModel(soil_type=soil_type)\n",
    "                    model.station = station\n",
    "                    model.train(\n",
    "                        valid_data.rename(columns={\n",
    "                            duration_column: 'duration',\n",
    "                            event_column: 'observed'\n",
    "                        }),\n",
    "                        'duration', \n",
    "                        'observed'\n",
    "                    )\n",
    "                    \n",
    "                    # Add to our models dictionary\n",
    "                    self.models[model_key] = model\n",
    "                    \n",
    "                    # Add to available soil types if not already there\n",
    "                    if soil_type not in self.available_soil_types:\n",
    "                        self.available_soil_types.append(soil_type)\n",
    "                    \n",
    "                    print(f\"Trained model for station {station}, soil type {soil_type} with {len(valid_data)} observations\")\n",
    "                else:\n",
    "                    print(f\"No valid data for station {station}, soil type {soil_type}\")\n",
    "            else:\n",
    "                print(f\"Missing columns for station {station}, soil type {soil_type}\")\n",
    "        \n",
    "        # Mark as fitted if we have any models\n",
    "        if self.models:\n",
    "            self.is_fitted = True\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def save(self, path, split_by_station=True):\n",
    "        \"\"\"\n",
    "        Save the FloodModel to disk.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        path : str\n",
    "            File path where the model should be saved\n",
    "        split_by_station : bool\n",
    "            If True, save each station's models in a separate file\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : FloodModel\n",
    "            Returns self for method chaining\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Model must be trained before saving\")\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        \n",
    "        # Split storage by station\n",
    "        if split_by_station and len(self.models) > 0:\n",
    "            # Extract base directory and filename without extension\n",
    "            base_dir = os.path.dirname(path)\n",
    "            base_name = os.path.splitext(os.path.basename(path))[0]\n",
    "            \n",
    "            # Create stations directory\n",
    "            stations_dir = os.path.join(base_dir, f\"{base_name}_stations\")\n",
    "            os.makedirs(stations_dir, exist_ok=True)\n",
    "            \n",
    "            print(f\"Starting split save: {len(self.stations)} stations with {len(self.models)} models...\")\n",
    "            \n",
    "            # Group models by station\n",
    "            station_models = {}\n",
    "            for model_key, model in self.models.items():\n",
    "                station = model_key.split('_')[0]\n",
    "                if station not in station_models:\n",
    "                    station_models[station] = {}\n",
    "                station_models[station][model_key] = model\n",
    "            \n",
    "            # Create a metadata model that references station files\n",
    "            meta_model = {\n",
    "                'is_fitted': True,\n",
    "                'units': self.units,\n",
    "                'soil_types': self.soil_types,\n",
    "                'stations': self.stations,\n",
    "                'available_soil_types': self.available_soil_types,\n",
    "                'station_paths': {}  # Will store paths to station model files\n",
    "            }\n",
    "            \n",
    "            # Save each station separately\n",
    "            saved_files = 0\n",
    "            for station, models in station_models.items():\n",
    "                station_path = os.path.join(stations_dir, f\"station_{station}.joblib\")\n",
    "                \n",
    "                # Display progress every 10 stations\n",
    "                if saved_files % 10 == 0:\n",
    "                    print(f\"Saving station {saved_files}/{len(station_models)}: {station} with {len(models)} models...\")\n",
    "                    \n",
    "                # Save station models with joblib\n",
    "                joblib.dump(models, station_path, compress=3)\n",
    "                \n",
    "                # Store the relative path in metadata\n",
    "                meta_model['station_paths'][station] = os.path.relpath(station_path, base_dir)\n",
    "                saved_files += 1\n",
    "                \n",
    "            # Save the metadata file\n",
    "            print(f\"Saving metadata to {path}...\")\n",
    "            joblib.dump(meta_model, path, compress=3)\n",
    "            print(f\"Successfully saved {saved_files} station files and metadata\")\n",
    "            \n",
    "        else:\n",
    "            # Traditional single-file save\n",
    "            print(f\"Starting to save {len(self.models)} models to {path}...\")\n",
    "            model_data = {\n",
    "                'models': self.models,\n",
    "                'is_fitted': self.is_fitted,\n",
    "                'units': self.units,\n",
    "                'soil_types': self.soil_types,\n",
    "                'stations': self.stations,\n",
    "                'available_soil_types': self.available_soil_types\n",
    "            }\n",
    "            joblib.dump(model_data, path, compress=3)\n",
    "            print(f\"Successfully saved model to {path}\")\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def load(self, path, lazy_load=True):\n",
    "        \"\"\"\n",
    "        Load a saved FloodModel from disk.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        path : str\n",
    "            File path to the saved model\n",
    "        lazy_load : bool\n",
    "            If True and model was saved with split_by_station=True, \n",
    "            only load station models when requested\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : FloodModel\n",
    "            Returns self with loaded models\n",
    "        \"\"\"\n",
    "        print(f\"Loading model from {path}...\")\n",
    "        \n",
    "        # Try to load model data\n",
    "        model_data = joblib.load(path)\n",
    "        \n",
    "        # Check if this is a split model (metadata file)\n",
    "        if isinstance(model_data, dict) and 'station_paths' in model_data:\n",
    "            # This is a split model - load metadata\n",
    "            self.is_fitted = model_data.get('is_fitted', False)\n",
    "            self.units = model_data.get('units', 'hours')\n",
    "            self.soil_types = model_data.get('soil_types', [])\n",
    "            self.stations = model_data.get('stations', [])\n",
    "            self.available_soil_types = model_data.get('available_soil_types', [])\n",
    "            \n",
    "            # Get base directory for relative paths\n",
    "            base_dir = os.path.dirname(path)\n",
    "            \n",
    "            if lazy_load:\n",
    "                # Create a proxy function for each station that will load data when needed\n",
    "                self.models = {}\n",
    "                print(f\"Lazy-loading enabled: Referenced {len(model_data['station_paths'])} stations\")\n",
    "                \n",
    "                # Store the station paths for later loading\n",
    "                self._station_paths = {\n",
    "                    station: os.path.join(base_dir, rel_path) \n",
    "                    for station, rel_path in model_data['station_paths'].items()\n",
    "                }\n",
    "            else:\n",
    "                # Load all station models immediately\n",
    "                self.models = {}\n",
    "                total_stations = len(model_data['station_paths'])\n",
    "                print(f\"Loading all {total_stations} station models...\")\n",
    "                \n",
    "                for i, (station, rel_path) in enumerate(model_data['station_paths'].items()):\n",
    "                    station_path = os.path.join(base_dir, rel_path)\n",
    "                    if i % 10 == 0:\n",
    "                        print(f\"Loading station {i+1}/{total_stations}: {station}...\")\n",
    "                    \n",
    "                    try:\n",
    "                        # Load the station models\n",
    "                        station_models = joblib.load(station_path)\n",
    "                        # Add to the main models dictionary\n",
    "                        self.models.update(station_models)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading station {station}: {e}\")\n",
    "        else:\n",
    "            # Traditional single-file model\n",
    "            self.models = model_data.get('models', {})\n",
    "            self.is_fitted = model_data.get('is_fitted', False)\n",
    "            self.units = model_data.get('units', 'hours')\n",
    "            self.soil_types = model_data.get('soil_types', [])\n",
    "            self.stations = model_data.get('stations', [])\n",
    "            self.available_soil_types = model_data.get('available_soil_types', [])\n",
    "        \n",
    "        print(f\"Model loaded with {len(self.stations)} stations\")\n",
    "        return self\n",
    "\n",
    "    def get_station_models(self, station):\n",
    "        \"\"\"\n",
    "        Get all models for a specific station.\n",
    "        Will load from disk if using lazy loading.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        station : str\n",
    "            Station identifier\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary of models for the station\n",
    "        \"\"\"\n",
    "        # Check if we're using lazy loading and need to load this station\n",
    "        if hasattr(self, '_station_paths') and station in self._station_paths:\n",
    "            # Station not loaded yet, load it now\n",
    "            station_path = self._station_paths[station]\n",
    "            print(f\"Loading station {station} models from {station_path}...\")\n",
    "            \n",
    "            try:\n",
    "                # Load the station models\n",
    "                station_models = joblib.load(station_path)\n",
    "                # Add to the main models dictionary\n",
    "                self.models.update(station_models)\n",
    "                # Return the loaded models for this station\n",
    "                return {k: v for k, v in station_models.items()}\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading station {station}: {e}\")\n",
    "                return {}\n",
    "        \n",
    "        # If not lazy loading or already loaded, filter existing models\n",
    "        return {k: v for k, v in self.models.items() if k.startswith(f\"{station}_\")}\n",
    "\n",
    "    def load_station(self, station, stations_dir):\n",
    "        \"\"\"\n",
    "        Load models for a specific station from the stations directory.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        station : str\n",
    "            Station identifier\n",
    "        stations_dir : str\n",
    "            Directory containing station model files\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary of loaded models for this station\n",
    "        \"\"\"\n",
    "        # Try different filename patterns\n",
    "        file_patterns = [\n",
    "            os.path.join(stations_dir, f\"{station}.joblib\"),\n",
    "            os.path.join(stations_dir, f\"station_{station}.joblib\")\n",
    "        ]\n",
    "        \n",
    "        loaded_models = {}\n",
    "        for file_path in file_patterns:\n",
    "            if os.path.exists(file_path):\n",
    "                try:\n",
    "                    print(f\"Loading station models from {file_path}\")\n",
    "                    station_models = joblib.load(file_path)\n",
    "                    \n",
    "                    # Add models to the main models dictionary\n",
    "                    if isinstance(station_models, dict):\n",
    "                        for model_key, model in station_models.items():\n",
    "                            self.models[model_key] = model\n",
    "                            loaded_models[model_key] = model\n",
    "                    \n",
    "                    # Return the loaded models\n",
    "                    return loaded_models\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading station file {file_path}: {e}\")\n",
    "        \n",
    "        print(f\"No valid model file found for station {station} in {stations_dir}\")\n",
    "        return {}\n",
    "    \n",
    "    def predict_proba(self, geodata, station_coverage, year):\n",
    "        \"\"\"\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Model must be trained before making predictions\")\n",
    "        \n",
    "        result_geodata = geodata.copy()\n",
    "        column_name = f'predictions_{year}'\n",
    "\n",
    "        soil_type_col = 'sediment'\n",
    "        if soil_type_col not in result_geodata.columns:\n",
    "            # Try to find a suitable column for soil types\n",
    "            possible_cols = [col for col in result_geodata.columns \n",
    "                        if 'soil' in col.lower() or 'type' in col.lower()]\n",
    "            if possible_cols:\n",
    "                soil_type_col = possible_cols[0]\n",
    "            else:\n",
    "                print(\"Could not find soil type column in geodata\")\n",
    "                return geodata\n",
    "         # Initialize prediction column\n",
    "        result_geodata[column_name] = None\n",
    "        \n",
    "        # If no station coverage provided, use all available stations with equal weight\n",
    "        # Spatial join to find which station coverage area each geometry falls into\n",
    "        # Ensure both GeoDataFrames have the same CRS\n",
    "        if result_geodata.crs != station_coverage.crs:\n",
    "            print(f\"Converting station_coverage from {station_coverage.crs} to {result_geodata.crs}\")\n",
    "            station_coverage = station_coverage.to_crs(result_geodata.crs)\n",
    "        \n",
    "        # Get the station ID column\n",
    "        station_id_col = 'station_id'\n",
    "        if station_id_col not in station_coverage.columns:\n",
    "            # Try to find a suitable station ID column\n",
    "            possible_cols = [col for col in station_coverage.columns \n",
    "                           if 'station' in col.lower() and 'id' in col.lower()]\n",
    "            if possible_cols:\n",
    "                station_id_col = possible_cols[0]\n",
    "            else:\n",
    "                print(\"Could not find station ID column in station_coverage\")\n",
    "                return geodata\n",
    "        \n",
    "        # Process each row in geodata\n",
    "        for idx, row in result_geodata.iterrows():\n",
    "            geometry = row.geometry\n",
    "            soil_type = row[soil_type_col]\n",
    "            \n",
    "            # Find which station coverage area this geometry intersects with\n",
    "            intersecting_stations = station_coverage[station_coverage.intersects(geometry)]\n",
    "            \n",
    "            if not intersecting_stations.empty:\n",
    "                # Extract first element if it's a compound soil type description\n",
    "                if isinstance(soil_type, str) and ' ' in soil_type:\n",
    "                    simple_type = soil_type.split(' ')[0]\n",
    "                else:\n",
    "                    simple_type = soil_type\n",
    "                    \n",
    "                # Get predictions from all intersecting stations and take the average\n",
    "                predictions = []\n",
    "                for _, station_row in intersecting_stations.iterrows():\n",
    "                    station = station_row[station_id_col]\n",
    "                    model_key = f\"{station}_{simple_type}\"\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    if model_key in self.models:\n",
    "                        try:\n",
    "                            model = self.models[model_key]\n",
    "                            predictions.append(model.predict(year))\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error predicting for {model_key}: {e}\")\n",
    "                \n",
    "                # Calculate the average prediction if we found any models\n",
    "                if predictions:\n",
    "                    result_geodata.at[idx, column_name] = sum(predictions) / len(predictions)\n",
    "                else:\n",
    "                    # No models found for this soil type at these stations, assign a default value\n",
    "                    default_value = min(0.2 + 0.05 * year, 0.0)\n",
    "                    result_geodata.at[idx, column_name] = default_value\n",
    "            else:\n",
    "                # Geometry doesn't intersect with any station coverage area\n",
    "                default_value = min(0.2 + 0.05 * year, 0.0)\n",
    "                result_geodata.at[idx, column_name] = default_value\n",
    "    \n",
    "        # Store raw probability values before percentage conversion\n",
    "        result_geodata[f'{column_name}_raw'] = result_geodata[column_name].copy()\n",
    "        \n",
    "        # Convert to percentage for visualization\n",
    "        result_geodata[column_name] = result_geodata[column_name] * 100\n",
    "        \n",
    "        return result_geodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aba_flooding.model as md\n",
    "import pandas as pd\n",
    "# import geopandas as gpd\n",
    "# import geo_utils as gu\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import cProfile\n",
    "import pstats\n",
    "from io import StringIO\n",
    "from aba_flooding.preprocess import load_saved_data\n",
    "from lifelines import KaplanMeierFitter\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "\n",
    "def process_station_file(file, processed_data_path, profile=False):\n",
    "    \"\"\"\n",
    "    Process a single station file - can be run in parallel\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (station ID, survival models dict, timing info dict)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        station_start_time = time.time()\n",
    "        \n",
    "        # Extract station ID from filename\n",
    "        station = file.replace(\"survival_data_\", \"\").replace(\".parquet\", \"\")\n",
    "        \n",
    "        # Load the data for the station\n",
    "        file_path = os.path.join(processed_data_path, file)\n",
    "        \n",
    "        load_start_time = time.time()\n",
    "        survival_df = load_saved_data(file_path)\n",
    "        load_time = time.time() - load_start_time\n",
    "        \n",
    "        if survival_df is None or survival_df.empty:\n",
    "            print(f\"Skipping empty data file for station {station}\")\n",
    "            return station, None, {\n",
    "                'station': station,\n",
    "                'status': 'skipped',\n",
    "                'reason': 'empty data'\n",
    "            }\n",
    "            \n",
    "        # Identify soil types in this dataframe\n",
    "        soil_types = set()\n",
    "        for column in survival_df.columns:\n",
    "            parts = column.split('_')\n",
    "            # Check if this is a station-soil column\n",
    "            if len(parts) >= 3 and parts[0] == station:\n",
    "                if parts[1] != \"WOG\":  # Skip WOG columns\n",
    "                    soil_types.add(parts[1])\n",
    "        \n",
    "        # Create station models\n",
    "        station_models = {}\n",
    "        \n",
    "        # Run profiling if enabled\n",
    "        if profile:\n",
    "            print(f\"\\nProfiling station {station}...\")\n",
    "            profiler = cProfile.Profile()\n",
    "            profiler.enable()\n",
    "            \n",
    "            # Process each soil type\n",
    "            for soil_type in soil_types:\n",
    "                # Create column names\n",
    "                duration_column = f\"{station}_{soil_type}_duration\"\n",
    "                event_column = f\"{station}_{soil_type}_observed\"\n",
    "                \n",
    "                # Check if columns exist\n",
    "                if duration_column in survival_df.columns and event_column in survival_df.columns:\n",
    "                    valid_data = survival_df[[duration_column, event_column]].dropna()\n",
    "                    \n",
    "                    if len(valid_data) > 0:\n",
    "                        # Create and train the model\n",
    "                        model = md.SurvivalModel(soil_type=soil_type)\n",
    "                        model.station = station\n",
    "                        model.train(\n",
    "                            valid_data.rename(columns={\n",
    "                                duration_column: 'duration',\n",
    "                                event_column: 'observed'\n",
    "                            }),\n",
    "                            'duration', \n",
    "                            'observed'\n",
    "                        )\n",
    "                        \n",
    "                        # Add to our local models dictionary\n",
    "                        model_key = f\"{station}_{soil_type}\"\n",
    "                        station_models[model_key] = model\n",
    "            \n",
    "            profiler.disable()\n",
    "            \n",
    "            # Print profile results\n",
    "            s = StringIO()\n",
    "            ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')\n",
    "            ps.print_stats(20)  # Top 20 functions by time\n",
    "            print(s.getvalue())\n",
    "        else:\n",
    "            # Add station to the model without profiling\n",
    "            train_start_time = time.time()\n",
    "            \n",
    "            # Process each soil type\n",
    "            for soil_type in soil_types:\n",
    "                # Create column names\n",
    "                duration_column = f\"{station}_{soil_type}_duration\"\n",
    "                event_column = f\"{station}_{soil_type}_observed\"\n",
    "                \n",
    "                # Check if columns exist\n",
    "                if duration_column in survival_df.columns and event_column in survival_df.columns:\n",
    "                    valid_data = survival_df[[duration_column, event_column]].dropna()\n",
    "                    \n",
    "                    if len(valid_data) > 0:\n",
    "                        # Create and train the model\n",
    "                        model = md.SurvivalModel(soil_type=soil_type)\n",
    "                        model.station = station\n",
    "                        model.train(\n",
    "                            valid_data.rename(columns={\n",
    "                                duration_column: 'duration',\n",
    "                                event_column: 'observed'\n",
    "                            }),\n",
    "                            'duration', \n",
    "                            'observed'\n",
    "                        )\n",
    "                        \n",
    "                        # Add to our local models dictionary\n",
    "                        model_key = f\"{station}_{soil_type}\"\n",
    "                        station_models[model_key] = model\n",
    "            \n",
    "            train_time = time.time() - train_start_time\n",
    "        \n",
    "        station_time = time.time() - station_start_time\n",
    "        \n",
    "        # Prepare timing info\n",
    "        timing = {\n",
    "            'station': station,\n",
    "            'total_time': station_time,\n",
    "            'load_time': load_time,\n",
    "            'train_time': station_time - load_time,\n",
    "            'soil_types': len(soil_types),\n",
    "            'models_trained': len(station_models)\n",
    "        }\n",
    "        \n",
    "        print(f\"Station {station}: {station_time:.2f}s (Load: {load_time:.2f}s, Train: {station_time - load_time:.2f}s)\")\n",
    "        \n",
    "        return station, station_models, timing\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing station file {file}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return file, None, {'status': 'error', 'error': str(e)}\n",
    "\n",
    "def train_all_models(output_path=\"models/flood_model.pkl\", profile=False, parallel=True, max_workers=None):\n",
    "    \"\"\"\n",
    "    Train survival models for all stations and soil types from processed parquet files.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    output_path : str\n",
    "        Path where the trained model will be saved\n",
    "    profile : bool\n",
    "        Whether to run detailed profiling for each station\n",
    "    parallel : bool\n",
    "        Whether to use parallel processing (default: True)\n",
    "    max_workers : int or None\n",
    "        Maximum number of parallel workers (default: CPU count - 1)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    FloodModel : Trained flood model\n",
    "    dict : Timing information\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    timing_info = {'total': 0, 'stations': {}}\n",
    "    \n",
    "    # Find all files in data/processed/\n",
    "    processed_data_path = os.path.join(os.getcwd(), \"data/processed/\")\n",
    "    station_files = []\n",
    "\n",
    "    # Check if directory exists\n",
    "    if os.path.exists(processed_data_path):\n",
    "        # List all files in the directory\n",
    "        files = os.listdir(processed_data_path)\n",
    "        \n",
    "        # Filter for parquet files\n",
    "        station_files = [f for f in files if f.startswith(\"survival_data_\") and f.endswith(\".parquet\")]\n",
    "        print(f\"Found {len(station_files)} station data files\")\n",
    "    else:\n",
    "        print(f\"Directory {processed_data_path} does not exist\")\n",
    "        return None, timing_info\n",
    "\n",
    "    # Create a new FloodModel\n",
    "    flood_model = md.FloodModel()\n",
    "\n",
    "    # Use parallel processing if enabled and more than one file\n",
    "    if parallel and len(station_files) > 1 and not profile:  # Skip parallel if profiling\n",
    "        # Determine number of workers - default is based on physical cores when possible\n",
    "        if max_workers is None:\n",
    "            try:\n",
    "                import psutil\n",
    "                # Get physical cores (10 in your case) not logical processors (16)\n",
    "                physical_cores = psutil.cpu_count(logical=False)\n",
    "                # Use physical cores minus 2 to leave room for system processes\n",
    "                max_workers = max(1, physical_cores - 2)\n",
    "            except (ImportError, AttributeError):\n",
    "                # If psutil isn't available, use a conservative default (about 60% of logical processors)\n",
    "                max_workers = max(1, int(multiprocessing.cpu_count() * 0.6))\n",
    "                \n",
    "            # Cap at 8 workers to prevent overloading\n",
    "            max_workers = min(8, max_workers)\n",
    "        \n",
    "        print(f\"Using parallel processing with {max_workers} workers\")\n",
    "        \n",
    "        # Create a pool of workers\n",
    "        with multiprocessing.Pool(processes=max_workers) as pool:\n",
    "            # Process all stations in parallel\n",
    "            process_func = partial(process_station_file, \n",
    "                                  processed_data_path=processed_data_path, \n",
    "                                  profile=profile)\n",
    "            \n",
    "            results = pool.map(process_func, station_files)\n",
    "            \n",
    "            # Collect results\n",
    "            for station, models, timing in results:\n",
    "                if models:\n",
    "                    # Add all models to the main flood model\n",
    "                    for model_key, model in models.items():\n",
    "                        flood_model.models[model_key] = model\n",
    "                    \n",
    "                    # Update station list if not already there\n",
    "                    if station not in flood_model.stations:\n",
    "                        flood_model.stations.append(station)\n",
    "                    \n",
    "                    # Update soil types\n",
    "                    for model_key in models:\n",
    "                        soil_type = model_key.split('_')[1]\n",
    "                        if soil_type not in flood_model.available_soil_types:\n",
    "                            flood_model.available_soil_types.append(soil_type)\n",
    "                \n",
    "                # Store timing info\n",
    "                if isinstance(timing, dict) and 'station' in timing:\n",
    "                    timing_info['stations'][station] = timing\n",
    "    else:\n",
    "        # Process files sequentially (original method)\n",
    "        for file in station_files:\n",
    "            station, models, timing = process_station_file(file, processed_data_path, profile)\n",
    "            \n",
    "            if models:\n",
    "                # Add all models to the main flood model\n",
    "                for model_key, model in models.items():\n",
    "                    flood_model.models[model_key] = model\n",
    "                \n",
    "                # Update station list if not already there\n",
    "                if station not in flood_model.stations:\n",
    "                    flood_model.stations.append(station)\n",
    "                \n",
    "                # Update soil types\n",
    "                for model_key in models:\n",
    "                    soil_type = model_key.split('_')[1]\n",
    "                    if soil_type not in flood_model.available_soil_types:\n",
    "                        flood_model.available_soil_types.append(soil_type)\n",
    "            \n",
    "            # Store timing info\n",
    "            if isinstance(timing, dict) and 'station' in timing:\n",
    "                timing_info['stations'][station] = timing\n",
    "\n",
    "    # If we've trained models then \n",
    "    if flood_model.models:\n",
    "        flood_model.is_fitted = True\n",
    "        total_time = time.time() - start_time\n",
    "        timing_info['total'] = total_time\n",
    "    else:\n",
    "        print(\"No models were successfully trained\")\n",
    "        total_time = time.time() - start_time\n",
    "        timing_info['total'] = total_time\n",
    "\n",
    "    return flood_model, timing_info\n",
    "\n",
    "\n",
    "def print_timing_report(timing_info):\n",
    "    \"\"\"Print a formatted timing report from timing information.\"\"\"\n",
    "    if not timing_info:\n",
    "        print(\"No timing information available\")\n",
    "        return\n",
    "        \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING PERFORMANCE REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total training time: {timing_info['total']:.2f} seconds\")\n",
    "    print(f\"Total stations: {len(timing_info['stations'])}\")\n",
    "    \n",
    "    if timing_info['stations']:\n",
    "        # Overall stats\n",
    "        station_times = [info['total_time'] for info in timing_info['stations'].values()]\n",
    "        avg_time = sum(station_times) / len(station_times)\n",
    "        max_time = max(station_times)\n",
    "        min_time = min(station_times)\n",
    "        \n",
    "        print(f\"\\nAverage time per station: {avg_time:.2f}s\")\n",
    "        print(f\"Fastest station: {min_time:.2f}s\")\n",
    "        print(f\"Slowest station: {max_time:.2f}s\")\n",
    "        \n",
    "        # Top 5 slowest stations\n",
    "        print(\"\\nTop 5 slowest stations:\")\n",
    "        sorted_stations = sorted(timing_info['stations'].items(), \n",
    "                                key=lambda x: x[1]['total_time'], \n",
    "                                reverse=True)\n",
    "        \n",
    "        for i, (station, info) in enumerate(sorted_stations[:5], 1):\n",
    "            print(f\"{i}. Station {station}: {info['total_time']:.2f}s - {info['soil_types']} soil types\")\n",
    "            \n",
    "        # Print save time if available\n",
    "        if 'save_time' in timing_info:\n",
    "            print(f\"\\nModel save time: {timing_info['save_time']:.2f}s\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "# Example usage script\n",
    "if __name__ == \"__main__\":\n",
    "    # Make sure the output directory exists\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    \n",
    "    # Enable detailed profiling if needed\n",
    "    enable_profiling = False\n",
    "    \n",
    "    # Enable parallel processing\n",
    "    enable_parallel = False\n",
    "    \n",
    "    # Set number of workers (None = auto)\n",
    "    num_workers = None\n",
    "    \n",
    "    # Train models for all stations\n",
    "    print(\"Training models for all stations...\")\n",
    "    training_start = time.time()\n",
    "    flood_model, timing_info = train_all_models(\n",
    "        \"models/flood_model.joblib\",\n",
    "        profile=enable_profiling,\n",
    "        parallel=enable_parallel, \n",
    "        max_workers=None  # Default to CPU count - 2\n",
    "    )\n",
    "\n",
    "    # Save using split storage\n",
    "    print(\"Saving model with split storage...\")\n",
    "    save_start = time.time()\n",
    "    flood_model.save(\"models/flood_model.joblib\", split_by_station=True)\n",
    "    save_time = time.time() - save_start\n",
    "    print(f\"Model saved in {save_time:.2f} seconds\")\n",
    "    # add save time to timing info\n",
    "    timing_info['save_time'] = save_time\n",
    "\n",
    "    # Print the timing report\n",
    "    print_timing_report(timing_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Test ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from aba_flooding.train import process_station_file\n",
    "from aba_flooding.model import FloodModel\n",
    "\n",
    "\n",
    "def inspect_model(train = False):\n",
    "    \"\"\"Inspect the trained flood model.\"\"\"\n",
    "    model_path = os.path.join(\"models\", \"flood_model.joblib\")\n",
    "    \n",
    "    \n",
    "    print(f\"Loading model from {model_path}...\")\n",
    "    try:\n",
    "        # Create a FloodModel instance first, then call load with the path parameter\n",
    "        model = FloodModel()\n",
    "        if train:\n",
    "            print(\"Training the model...\")\n",
    "            if train:\n",
    "                \n",
    "                # Fix: Pass string filename first, not the model object\n",
    "                station_file = \"data/processed/survival_data_05005.parquet\"\n",
    "                station_id = \"05005\"\n",
    "                station, station_models, timing = process_station_file(f\"survival_data_{station_id}.parquet\", \n",
    "                                                                    os.path.dirname(station_file), \n",
    "                                                                    False)\n",
    "                \n",
    "                # Only try to add models if they were successfully created\n",
    "                if station_models:\n",
    "                    for model_key, survival_model in station_models.items():\n",
    "                        model.models[model_key] = survival_model\n",
    "                    model.stations.append(station)\n",
    "                    \n",
    "                    # Update available soil types\n",
    "                    for model_key in station_models:\n",
    "                        soil_type = model_key.split('_')[1]\n",
    "                        if soil_type not in model.available_soil_types:\n",
    "                            model.available_soil_types.append(soil_type)\n",
    "            else:\n",
    "                print(f\"No models were created for station {station_id}\")\n",
    "        else:\n",
    "            if not os.path.exists(model_path):\n",
    "                print(f\"Model file not found: {model_path}\")\n",
    "                return\n",
    "            model.load(path=model_path)\n",
    "        \n",
    "        # Inspect specific station\n",
    "        inspect_station(model, \"05005\")\n",
    "        \n",
    "        print(\"\\n== Model Summary ==\")\n",
    "        print(f\"Number of stations: {len(model.stations)}\")\n",
    "        print(f\"Total models: {len(model.models)}\")\n",
    "        print(f\"Number of available soil types: {len(model.available_soil_types)}\")\n",
    "        print(f\"Available soil types: {model.available_soil_types}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "def inspect_station(model, station_id):\n",
    "    \"\"\"\n",
    "    Inspect a specific station's models in detail\n",
    "    \n",
    "    Parameters:\n",
    "    -----------    \n",
    "    model : FloodModel\n",
    "        The loaded FloodModel instance\n",
    "    station_id : str\n",
    "        The station ID to inspect\n",
    "    \"\"\"\n",
    "    print(f\"\\n==== INSPECTING STATION {station_id} ====\")\n",
    "    \n",
    "    # Check if station exists in the model\n",
    "    if station_id not in model.stations:\n",
    "        print(f\"Station {station_id} not found in model\")\n",
    "        return\n",
    "    \n",
    "    # For debugging, let's examine the model structure\n",
    "    print(f\"Available model attributes: {[attr for attr in dir(model) if not attr.startswith('_')]}\")\n",
    "\n",
    "    # Try using the get_station_models method if available\n",
    "    if hasattr(model, 'get_station_models'):\n",
    "        print(f\"Using get_station_models to load models for station {station_id}...\")\n",
    "        try:\n",
    "            station_models = model.get_station_models(station_id)\n",
    "            if station_models:\n",
    "                print(f\"Successfully loaded {len(station_models)} models for station {station_id}\")\n",
    "                \n",
    "                # Plot survival curves for this station\n",
    "                plot_survival_curves(station_models, station_id)\n",
    "                \n",
    "                # Report on each model\n",
    "                for key, survival_model in station_models.items():\n",
    "                    # Extract just the soil type part (remove station prefix if present)\n",
    "                    if key.startswith(f\"{station_id}_\"):\n",
    "                        soil_type = key.replace(f\"{station_id}_\", \"\")\n",
    "                    else:\n",
    "                        soil_type = key\n",
    "                        \n",
    "                    print(f\"\\nSoil type: {soil_type}\")\n",
    "                    \n",
    "                    # Print model attributes for debugging\n",
    "                    model_attrs = [attr for attr in dir(survival_model) if not attr.startswith('_')]\n",
    "                    print(f\"  - Model attributes: {model_attrs}\")\n",
    "                    \n",
    "                    # Check if model is fitted\n",
    "                    is_fitted = survival_model.is_fitted if hasattr(survival_model, 'is_fitted') else False\n",
    "                    print(f\"  - Is fitted: {is_fitted}\")\n",
    "                    \n",
    "                    # Check for model attribute that might contain the fitted estimator\n",
    "                    if hasattr(survival_model, 'model') and survival_model.model is not None:\n",
    "                        print(f\"  - Has model object: Yes\")\n",
    "                        if hasattr(survival_model.model, 'median_survival_time_'):\n",
    "                            print(f\"  - Median survival time: {survival_model.model.median_survival_time_}\")\n",
    "                    else:\n",
    "                        print(f\"  - Has model object: No\")\n",
    "                    \n",
    "                    # Test different time intervals - both in years and days\n",
    "                    time_periods = [\n",
    "                        {'value': 10, 'unit': 'days'},\n",
    "                        {'value': 150, 'unit': 'days'},\n",
    "                        {'value': 1, 'unit': 'years'},\n",
    "                        {'value': 5, 'unit': 'years'},\n",
    "                        {'value': 10, 'unit': 'years'}\n",
    "                    ]\n",
    "                    print(f\"  - Prediction results:\")\n",
    "                    for period in time_periods:\n",
    "                        try:\n",
    "                            if hasattr(survival_model, 'predict_proba'):\n",
    "                                # Convert days to years for prediction if needed\n",
    "                                t_value = period['value']\n",
    "                                if period['unit'] == 'days':\n",
    "                                    t_years = t_value / 365.25  # Convert to years\n",
    "                                else:\n",
    "                                    t_years = t_value\n",
    "                                \n",
    "\n",
    "                                surv_prob = survival_model.predict_proba(t_years)\n",
    "                                # Flood probability is 1-survival probability\n",
    "                                flood_prob = (1-surv_prob)*100 if isinstance(surv_prob, (int, float)) else None\n",
    "                                print(f\"    - At {period['value']} {period['unit']}: {flood_prob:.2f}% flood probability\" \n",
    "                                      if flood_prob is not None else f\"    - At {period['value']} {period['unit']}: No valid prediction\")\n",
    "                            else:\n",
    "                                print(f\"    - At {period['value']} {period['unit']}: predict_proba method not available\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"    - At {period['value']} {period['unit']}: Error - {str(e)}\")\n",
    "                \n",
    "                return  # Exit once we've used get_station_models successfully\n",
    "            else:\n",
    "                print(f\"get_station_models returned empty for station {station_id}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error using get_station_models: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Check for station model files directly\n",
    "    print(\"\\nChecking for station model files...\")\n",
    "    model_dir = os.path.join(\"models\", \"stations\")\n",
    "    station_file = os.path.join(model_dir, f\"{station_id}.joblib\")\n",
    "    \n",
    "    if os.path.exists(station_file):\n",
    "        print(f\"Found station file at {station_file}\")\n",
    "        # You could add code here to load and inspect this file directly\n",
    "    else:\n",
    "        print(f\"No station file found at {station_file}\")\n",
    "    \n",
    "    # List all available station files\n",
    "    if os.path.exists(model_dir):\n",
    "        station_files = [f for f in os.listdir(model_dir) if f.endswith('.joblib')]\n",
    "        if station_files:\n",
    "            print(f\"\\nAvailable station files ({len(station_files)} total):\")\n",
    "            for i, file in enumerate(sorted(station_files)[:10]):\n",
    "                print(f\"  - {file}\")\n",
    "            if len(station_files) > 10:\n",
    "                print(f\"  - ... and {len(station_files) - 10} more\")\n",
    "        else:\n",
    "            print(\"No station files found in models/stations directory\")\n",
    "    else:\n",
    "        print(f\"Directory {model_dir} does not exist\")\n",
    "    \n",
    "    print(\"\\n==== STATION INSPECTION COMPLETE ====\")\n",
    "\n",
    "def plot_survival_curves(station_models, station_id):\n",
    "    \"\"\"\n",
    "    Plot survival curves for models of a specific station\n",
    "    \n",
    "    Parameters:\n",
    "    -----------    \n",
    "    station_models : dict\n",
    "        Dictionary of soil type -> survival model\n",
    "    station_id : str\n",
    "        The station ID\n",
    "    \"\"\"\n",
    "    print(\"\\n== Creating Survival Curve Plots ==\")\n",
    "    \n",
    "    if not station_models:\n",
    "        print(\"No models available to plot\")\n",
    "        return\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    plot_dir = os.path.join(\"outputs\", \"plots\", \"inspect_model\")\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    \n",
    "    # Different time scales to plot\n",
    "    time_ranges = [\n",
    "        {\"max\": 1, \"label\": \"1 Year\", \"filename\": \"1year\"},\n",
    "        {\"max\": 5, \"label\": \"5 Years\", \"filename\": \"5years\"},\n",
    "        {\"max\": 10, \"label\": \"10 Years\", \"filename\": \"10years\"}\n",
    "    ]\n",
    "    \n",
    "    # For each time range, create a separate plot\n",
    "    for time_range in time_ranges:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Generate time points (convert to days for x-axis display)\n",
    "        t_max = time_range[\"max\"]  # in years\n",
    "        t = np.linspace(0.01, t_max, 100)  # 100 points from 0.01 to max years (avoid 0)\n",
    "        t_days = t * 365.25  # convert to days for display\n",
    "        \n",
    "        soil_types_plotted = 0\n",
    "        \n",
    "        # Plot each soil type\n",
    "        for key, survival_model in station_models.items():\n",
    "            # Extract soil type for the legend\n",
    "            if key.startswith(f\"{station_id}_\"):\n",
    "                soil_type = key.replace(f\"{station_id}_\", \"\")\n",
    "            else:\n",
    "                soil_type = key\n",
    "                \n",
    "            try:\n",
    "                if hasattr(survival_model, 'predict_proba') and hasattr(survival_model, 'is_fitted') and survival_model.is_fitted:\n",
    "                    # Get survival probabilities at each time point\n",
    "                    survival_probs = [survival_model.predict_proba(time_point) for time_point in t]\n",
    "                    \n",
    "                    # Convert to flood probabilities\n",
    "                    flood_probs = [1 - prob for prob in survival_probs]\n",
    "                    \n",
    "                    # Plot the flood probability curve\n",
    "                    plt.plot(t_days, flood_probs, label=f\"Soil: {soil_type}\", linewidth=2)\n",
    "                    soil_types_plotted += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  Error plotting model for soil type {soil_type}: {e}\")\n",
    "        \n",
    "        if soil_types_plotted > 0:\n",
    "            plt.xlabel('Time (days)')\n",
    "            plt.ylabel('Flood Probability')\n",
    "            plt.title(f'Flood Probability Curves for Station {station_id} ({time_range[\"label\"]})')\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.legend()\n",
    "            \n",
    "            # Save the plot\n",
    "            filename = f\"station_{station_id}_flood_prob_{time_range['filename']}.png\"\n",
    "            filepath = os.path.join(plot_dir, filename)\n",
    "            plt.savefig(filepath)\n",
    "            print(f\"  Saved plot: {filepath}\")\n",
    "            \n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.close()\n",
    "            print(f\"  No valid models to plot for {time_range['label']} time range\")\n",
    "    \n",
    "    print(\"== Plotting complete ==\")\n",
    "\n",
    "import pandas as pd\n",
    "from lifelines import KaplanMeierFitter, WeibullFitter, ExponentialFitter, LogNormalFitter\n",
    "from sksurv.nonparametric import kaplan_meier_estimator\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #inspect_model(True)\n",
    "\n",
    "    df = pd.read_parquet(\"data/processed/survival_data_05109.parquet\")\n",
    "\n",
    "    print(df.columns)\n",
    "\n",
    "    km = KaplanMeierFitter()\n",
    "    \n",
    "    km.fit(durations=df['05109_HI_TTE'],event_observed=df['05109_HI_observed'])\n",
    "    \n",
    "    df['05109_HI_observed'] = df['05109_HI_observed'].astype(bool)\n",
    "\n",
    "    time, survival_prob, conf_int = kaplan_meier_estimator(df['05109_HI_observed'], df['05109_HI_duration'], conf_type=\"log-log\")\n",
    "\n",
    "    # Ensure the directory exists before saving the plot\n",
    "    output_dir = os.path.join('outputs', 'plots', 'inspect_model')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    plt.step(time, survival_prob, where=\"post\")\n",
    "    plt.fill_between(time, conf_int[0], conf_int[1], alpha=0.25, step=\"post\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.ylabel(r\"est. probability of survival $\\hat{S}(t)$\")\n",
    "    plt.xlabel(\"time $t$\")\n",
    "    plt.savefig(os.path.join(output_dir, \"km_plot.png\"))\n",
    "\n",
    "    print(df['05109_HI_observed'].value_counts())\n",
    "    print(df['05109_HI_duration'].describe())\n",
    "    event_rows = df[df['05109_HI_observed'] == 1]\n",
    "    print(f\"\\nFound {len(event_rows)} events\")\n",
    "    if len(event_rows) > 0:\n",
    "        print(\"\\nFirst 5 events:\")\n",
    "        print(event_rows.head())\n",
    "    else:\n",
    "        print(\"No events found! All observations are censored.\")\n",
    "\n",
    "\n",
    "    # Try plotting the cumulative hazard (might show the pattern better)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    km.plot_cumulative_density()\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Cumulative density\")\n",
    "    plt.savefig('outputs/plots/inspect_model/cumulative_density.png')\n",
    "\n",
    "    # Check for issues in the duration data\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(df['05109_HI_duration'], bins=50) \n",
    "    plt.title(\"Distribution of Duration Values\") \n",
    "    plt.savefig('outputs/plots/inspect_model/duration_hist.png')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(df['05109_WOG_HI'])\n",
    "    plt.savefig(\"outputs/plots/inspect_model/ss21.png\")\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "\n",
    "    df2 = pd.read_parquet(\"data/raw/precipitation_imputed_data.parquet\")\n",
    "    df2 = df2.clip(lower=0, upper=60)\n",
    "    print(df2['05109'].isnull().sum())\n",
    "    print(len(df2['05109']))\n",
    "    print(len(df))\n",
    "    #inspect_model()\n",
    "\n",
    "\n",
    "\n",
    "    # DIAGNOSTIC SECTION\n",
    "    print(\"\\n=== DIAGNOSTIC INFORMATION ===\")\n",
    "    event_rate = df['05109_HI_observed'].mean()\n",
    "    print(f\"Event rate: {event_rate:.4f} ({event_rate*100:.2f}%)\")\n",
    "\n",
    "    # SOLUTION 1: Try plotting with CONSISTENT variables\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    km_tte = KaplanMeierFitter()\n",
    "    km_tte.fit(durations=df['05109_HI_TTE'], event_observed=df['05109_HI_observed'])\n",
    "    km_tte.plot_cumulative_density()\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Cumulative Incidence (using TTE values)\")\n",
    "    plt.savefig('outputs/plots/inspect_model/cumulative_density_tte.png')\n",
    "\n",
    "    # SOLUTION 2: Try duration with events correctly marked\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    km_dur = KaplanMeierFitter()\n",
    "    km_dur.fit(durations=df['05109_HI_duration'], event_observed=df['05109_HI_observed'])\n",
    "    km_dur.plot_cumulative_density()\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Cumulative Incidence (using duration values)\")\n",
    "    plt.savefig('outputs/plots/inspect_model/cumulative_density_duration.png')\n",
    "\n",
    "\n",
    "    # SOLUTION 4: Check for time window issues\n",
    "    evenHI_by_time = df['05109_HI_observed'].rolling(window=1000).mean()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(evenHI_by_time)\n",
    "    plt.title(\"Event Rate Over Time (Moving Average)\")\n",
    "    plt.savefig('outputs/plots/inspect_model/event_rate_time.png')\n",
    "\n",
    "    # Create sksurv-compatible structured array\n",
    "    y = np.zeros(len(df), dtype=[('event', bool), ('time', float)])\n",
    "    y['event'] = df['05109_HI_observed'].values\n",
    "    y['time'] = df['05109_HI_duration'].values\n",
    "\n",
    "    print(\"\\nEvent time analysis:\")\n",
    "    event_durations = df[df['05109_HI_observed'] == 1]['05109_HI_duration'].describe()\n",
    "    print(f\"Event durations: {event_durations}\")\n",
    "    print(f\"Max duration overall: {df['05109_HI_duration'].max()}\")\n",
    "    print(f\"Events at max duration: {sum((df['05109_HI_observed'] == 1) & (df['05109_HI_duration'] == df['05109_HI_duration'].max()))}\")\n",
    "    \n",
    "\n",
    "    test = WeibullFitter()\n",
    "    test.fit(df['05109_HI_duration'], df['05109_HI_observed'])\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    test.plot_cumulative_density()\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Cumulative Incidence (Weibull)\")\n",
    "    plt.savefig(os.path.join('outputs', 'plots', 'inspect_model', 'cumulative_density_weibull.png'))\n",
    "    print(f\"Weibull parameters: {test.lambda_}, {test.rho_}\")\n",
    "    print(f\"Weibull median survival time: {test.median_survival_time_}\")\n",
    "    print(f\"Weibull AIC: {test.AIC_}\")\n",
    "    print(f\"Weibull BIC: {test.BIC_}\")\n",
    "\n",
    "    test = ExponentialFitter()\n",
    "    test.fit(df['05109_HI_duration'], df['05109_HI_observed'])\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    test.plot_cumulative_density()\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Cumulative Incidence (Exponential)\")\n",
    "    plt.savefig(os.path.join('outputs', 'plots', 'inspect_model', 'cumulative_density_exponential.png'))\n",
    "    print(f\"Exponential parameters: {test.lambda_}\")\n",
    "    print(f\"Exponential median survival time: {test.median_survival_time_}\")\n",
    "    print(f\"Exponential AIC: {test.AIC_}\")\n",
    "    print(f\"Exponential BIC: {test.BIC_}\")\n",
    "\n",
    "    test = LogNormalFitter()\n",
    "    test.fit(df['05109_HI_duration'], df['05109_HI_observed'])\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    test.plot_cumulative_density()\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Cumulative Incidence (LogNormal)\")\n",
    "    plt.savefig(os.path.join('outputs', 'plots', 'inspect_model', 'cumulative_density_lognormal.png'))\n",
    "    print(f\"LogNormal parameters: {test.mu_}, {test.sigma_}\")\n",
    "    print(f\"LogNormal median survival time: {test.median_survival_time_}\")\n",
    "    print(f\"LogNormal AIC: {test.AIC_}\")\n",
    "    print(f\"LogNormal BIC: {test.BIC_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import time\n",
    "import json  # Add for debugging GeoJSON structure\n",
    "\n",
    "from aba_flooding.geo_utils import load_terrain_data, gdf_to_geojson, wgs84_to_web_mercator, load_geojson, load_gpkg, load_terrain_data, gdf_to_geojson\n",
    "from aba_flooding.model import FloodModel\n",
    "\n",
    "from bokeh.models import ColorBar\n",
    "\n",
    "from bokeh.palettes import Viridis256, Category10\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_file, show\n",
    "from bokeh.models import GeoJSONDataSource, HoverTool, CheckboxGroup, CustomJS\n",
    "from bokeh.models import WMTSTileSource, Column, Slider\n",
    "from bokeh.transform import linear_cmap\n",
    "from bokeh.layouts import column\n",
    "\n",
    "\n",
    "MODEL_PATH = \"models/\"\n",
    "MODEL_NAME = \"flood_model.joblib\"\n",
    "\n",
    "def load_models(model_path):\n",
    "    \"\"\"Load a trained FloodModel from file, including split station files if available.\"\"\"\n",
    "    try:\n",
    "        print(f\"Loading model from {model_path}...\")\n",
    "        # Create a FloodModel instance first, then call load as instance method\n",
    "        model = FloodModel()\n",
    "        model.load(path=model_path)\n",
    "        \n",
    "        # Check if we need to load split station models\n",
    "        if len(model.models) == 0 and len(model.stations) > 0:\n",
    "            print(f\"Main model has stations but no models. Looking for split station files...\")\n",
    "            \n",
    "            # Check for a stations directory in the same location as the model file\n",
    "            model_dir = os.path.dirname(model_path)\n",
    "            stations_dir = os.path.join(model_dir, \"flood_model_stations\")\n",
    "            \n",
    "            # Also check for the _stations directory format used in train.py\n",
    "            if not os.path.exists(stations_dir):\n",
    "                base_name = os.path.splitext(os.path.basename(model_path))[0]\n",
    "                stations_dir = os.path.join(model_dir, f\"{base_name}_stations\")\n",
    "            \n",
    "            if os.path.exists(stations_dir):\n",
    "                print(f\"Found stations directory: {stations_dir}\")\n",
    "                loaded_count = 0\n",
    "                \n",
    "                # Load each station file\n",
    "                for station in model.stations:\n",
    "                    # Try both naming patterns\n",
    "                    station_files = [\n",
    "                        # os.path.join(stations_dir, f\"{station}.joblib\"),   # Original pattern\n",
    "                        os.path.join(stations_dir, f\"station_{station}.joblib\")  # Pattern from train.py\n",
    "                    ]\n",
    "                    \n",
    "                    station_file = None\n",
    "                    for potential_file in station_files:\n",
    "                        if os.path.exists(potential_file):\n",
    "                            station_file = potential_file\n",
    "                            break\n",
    "                    \n",
    "                    if station_file:\n",
    "                        try:\n",
    "                            print(f\"Loading station model from: {station_file}\")\n",
    "                            station_models = model.load_station(station, os.path.dirname(station_file))\n",
    "                            loaded_count += len(station_models) if station_models else 0\n",
    "                        except Exception as e:\n",
    "                            print(f\"ERROR loading station {station}: {e}\")\n",
    "                    else:\n",
    "                        print(f\"No file found for station {station}, tried patterns: {station_files}\")\n",
    "                \n",
    "                print(f\"Loaded {loaded_count} models for {len(model.stations)} stations\")\n",
    "            else:\n",
    "                print(f\"No stations directory found at {stations_dir}\")\n",
    "        \n",
    "        print(f\"Model loaded successfully with {len(model.models)} models across {len(model.stations)} stations\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        exit(1)\n",
    "        # return None\n",
    "\n",
    "def repair_geometries(gdf):\n",
    "    \"\"\"Repair invalid geometries in a GeoDataFrame\"\"\"\n",
    "    invalid_count = sum(~gdf.geometry.is_valid)\n",
    "    if invalid_count > 0:\n",
    "        print(f\"Fixing {invalid_count} invalid geometries\")\n",
    "        # buffer(0) is a common trick to fix many geometry issues\n",
    "        gdf.geometry = gdf.geometry.apply(lambda geom: geom.buffer(0) if not geom.is_valid else geom)\n",
    "        still_invalid = sum(~gdf.geometry.is_valid)\n",
    "        if still_invalid > 0:\n",
    "            print(f\"Warning: {still_invalid} geometries still invalid after repair\")\n",
    "    return gdf\n",
    "\n",
    "def init_map():\n",
    "    \"\"\"Initialize a Bokeh map with terrain data, sediment layers, and flood risk predictions using FloodModel.\"\"\"\n",
    "\n",
    "    # Load sediment data\n",
    "    print(\"Loading sediment data...\")\n",
    "    try:\n",
    "        # Uncomment the line below to use full Denmark dataset\n",
    "        sediment_data = load_terrain_data(\"Sediment_wgs84.geojson\")\n",
    "        # Comment out the line below when using full dataset\n",
    "        # sediment_data = load_terrain_data(\"Sediment.geojson\")\n",
    "\n",
    "        print(f\"Loaded sediment data with CRS: {sediment_data.crs}\")\n",
    "        print(f\"Sediment data size before simplification: {len(sediment_data)} polygons\")\n",
    "        \n",
    "        # Fix: Better CRS comparison and conversion\n",
    "        target_crs = \"EPSG:3857\"\n",
    "        if sediment_data.crs is None:\n",
    "            print(\"Warning: Sediment data has no CRS, assuming WGS84\")\n",
    "            sediment_data.crs = \"EPSG:4326\"\n",
    "            sediment_data = repair_geometries(sediment_data)\n",
    "            # Simplify geometries before conversion to reduce payload size\n",
    "            print(\"Simplifying geometries...\")\n",
    "            sediment_data = sediment_data.copy()\n",
    "            sediment_data.geometry = sediment_data.geometry.simplify(tolerance=50)\n",
    "            sediment_mercator = sediment_data.to_crs(target_crs)\n",
    "        elif str(sediment_data.crs).upper() != target_crs:\n",
    "            print(f\"Converting sediment data from {sediment_data.crs} to {target_crs} Web Mercator\")\n",
    "            # Fix any invalid geometries during conversion\n",
    "            sediment_data = repair_geometries(sediment_data)\n",
    "            # Simplify geometries before conversion to reduce payload size\n",
    "            print(\"Simplifying geometries...\")\n",
    "            sediment_data = sediment_data.copy()\n",
    "            sediment_data.geometry = sediment_data.geometry.simplify(tolerance=50)\n",
    "            sediment_mercator = sediment_data.to_crs(target_crs)\n",
    "        else:\n",
    "            print(\"Sediment data already in Web Mercator projection\")\n",
    "            # Still simplify geometries for better performance\n",
    "            print(\"Simplifying geometries...\")\n",
    "            sediment_data = sediment_data.copy()\n",
    "            sediment_data.geometry = sediment_data.geometry.simplify(tolerance=50)\n",
    "            sediment_mercator = sediment_data\n",
    "        \n",
    "        print(f\"Sediment data size after simplification: {len(sediment_mercator)} polygons\")\n",
    "        print(f\"Sediment data bounds: {sediment_mercator.total_bounds}\")\n",
    "            \n",
    "        has_sediment_data = True\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load sediment data: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        has_sediment_data = False\n",
    "        exit(1)\n",
    "    \n",
    "    # Prepare map figure\n",
    "    denmark_bounds_x = (670000, 1500000)\n",
    "    denmark_bounds_y = (7000000, 8170000)\n",
    "    p = figure(title=\"Flood Risk Map\", \n",
    "               x_axis_type=\"mercator\", y_axis_type=\"mercator\",\n",
    "               x_range=denmark_bounds_x, y_range=denmark_bounds_y,\n",
    "               tools=\"pan,wheel_zoom,box_zoom,reset,save\",\n",
    "               width=1200, height=900)\n",
    "    \n",
    "    # Add base map tiles\n",
    "    cartodb_positron = WMTSTileSource(\n",
    "        url='https://tiles.basemaps.cartocdn.com/light_all/{z}/{x}/{y}.png',\n",
    "        attribution=' OpenStreetMap contributors,  CartoDB'\n",
    "    )\n",
    "    p.add_tile(cartodb_positron)\n",
    "    \n",
    "    # Create precipitation coverage layer\n",
    "    precipitation_layer = None\n",
    "    station_layer = None\n",
    "    \n",
    "    # Load coverage data\n",
    "    try:\n",
    "        print(\"Loading precipitation coverage data...\")\n",
    "        coverage_data = load_geojson(\"precipitation_coverage.geojson\")\n",
    "        \n",
    "        if coverage_data is not None and not coverage_data.empty:\n",
    "            # Ensure data is in Web Mercator projection\n",
    "            if coverage_data.crs != \"EPSG:3857\":\n",
    "                coverage_mercator = coverage_data.to_crs(epsg=3857)\n",
    "            else:\n",
    "                coverage_mercator = coverage_data\n",
    "            \n",
    "            # Convert to GeoJSON for Bokeh\n",
    "            coverage_geojson = gdf_to_geojson(coverage_mercator)\n",
    "            coverage_source = GeoJSONDataSource(geojson=coverage_geojson)\n",
    "            \n",
    "            # Add precipitation coverage polygons\n",
    "            precipitation_layer = p.patches(\n",
    "                'xs', 'ys',\n",
    "                source=coverage_source,\n",
    "                fill_color='blue',\n",
    "                fill_alpha=0.2,\n",
    "                line_color='blue',\n",
    "                line_width=1,\n",
    "                legend_label=\"Precipitation Coverage\"\n",
    "            )\n",
    "            \n",
    "            # Create a hover tool for precipitation areas\n",
    "            precip_hover = HoverTool(\n",
    "                tooltips=[\n",
    "                    (\"Station ID\", \"@station_id\"),\n",
    "                    (\"Avg Precipitation\", \"@avg_precipitation{0.0} mm\")\n",
    "                ],\n",
    "                renderers=[precipitation_layer]\n",
    "            )\n",
    "            p.add_tools(precip_hover)\n",
    "            \n",
    "            # Extract station points (centroids of coverage areas) for visualization\n",
    "            stations_gdf = gpd.GeoDataFrame(\n",
    "                coverage_mercator.copy(),\n",
    "                geometry=coverage_mercator.geometry.centroid,\n",
    "                crs=coverage_mercator.crs\n",
    "            )\n",
    "            \n",
    "            # Convert station points to GeoJSON\n",
    "            stations_geojson = gdf_to_geojson(stations_gdf)\n",
    "            stations_source = GeoJSONDataSource(geojson=stations_geojson)\n",
    "            \n",
    "            # Add station points\n",
    "            station_layer = p.circle(\n",
    "                'x', 'y',\n",
    "                source=stations_source,\n",
    "                size=8,\n",
    "                color='blue',\n",
    "                fill_alpha=1.0,\n",
    "                line_color='white',\n",
    "                line_width=1\n",
    "            )\n",
    "            \n",
    "            print(f\"Successfully loaded precipitation coverage with {len(coverage_mercator)} areas\")\n",
    "        else:\n",
    "            print(\"No precipitation coverage data found or it's empty\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load precipitation coverage: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    # Add sediment layer if available\n",
    "    sediment_layer = None\n",
    "    if has_sediment_data:\n",
    "        try:\n",
    "            print(\"Converting sediment data to GeoJSON...\")\n",
    "            sediment_geojson = gdf_to_geojson(sediment_mercator)\n",
    "            \n",
    "            # Debug: Check the size of the GeoJSON payload\n",
    "            geojson_size_mb = len(sediment_geojson) / (1024 * 1024)\n",
    "            print(f\"GeoJSON payload size: {geojson_size_mb:.2f} MB\")\n",
    "            \n",
    "            if geojson_size_mb > 50:  # If payload is very large, consider URL option\n",
    "                print(\"Warning: GeoJSON payload is very large. Consider using file-based GeoJSON.\")\n",
    "                # Option to save and load via URL instead of inlining\n",
    "                # os.makedirs(\"data\", exist_ok=True)\n",
    "                # with open(\"data/sediment.geojson\", \"w\") as f:\n",
    "                #     f.write(sediment_geojson)\n",
    "                # sediment_source = GeoJSONDataSource(url=\"data/sediment.geojson\")\n",
    "            \n",
    "            # Debug: Check if GeoJSON has the expected fields\n",
    "            sample_feature = json.loads(sediment_geojson)[\"features\"][0] if \"features\" in json.loads(sediment_geojson) else None\n",
    "            if sample_feature:\n",
    "                print(f\"GeoJSON feature properties: {list(sample_feature['properties'].keys())}\")\n",
    "            \n",
    "            sediment_source = GeoJSONDataSource(geojson=sediment_geojson)\n",
    "            sediment_layer = p.patches('xs', 'ys', source=sediment_source,\n",
    "                                    fill_color='brown', fill_alpha=0.4,\n",
    "                                    line_color='black', line_width=0.2,\n",
    "                                    legend_label=\"Sediment\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR creating sediment layer: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Prepare FloodModel predictions\n",
    "    flood_layer = None\n",
    "    year_slider = Slider(start=0, end=10, value=0, step=1, title=\"Years into future\")\n",
    "    if has_sediment_data:\n",
    "        try:            \n",
    "            # Train FloodModel with all soil types from sediment data\n",
    "            if MODEL_NAME in os.listdir(MODEL_PATH):\n",
    "                flood_model = load_models(MODEL_PATH + MODEL_NAME)\n",
    "            else:\n",
    "                print(\"No model found, GO train some by running train.py\")\n",
    "                # print(\"Training new flood models...\")\n",
    "                # Train models for all soil types\n",
    "                # flood_model = train_all_models(soil_types, stationId)\n",
    "                # Plot models for available soil types\n",
    "                #flood_model.plot_all(save=True)\n",
    "\n",
    "            # Precompute predictions for all years\n",
    "            print(f\"Starting predictions on {len(sediment_mercator)} polygons...\")\n",
    "            sediment_with_predictions = sediment_mercator.copy()\n",
    "            for year in range(0, 2):\n",
    "                start_time = time.time()\n",
    "                sediment_with_predictions = flood_model.predict_proba(sediment_with_predictions, station_coverage=stations_gdf, year=year)\n",
    "                end_time = time.time()\n",
    "                prediction_time = end_time - start_time\n",
    "                print(f\"Predicted Year {year} in {prediction_time:.2f} seconds\")\n",
    "            \n",
    "            # Convert to GeoJSON data source\n",
    "            print(\"Converting predictions to GeoJSON...\")\n",
    "            flood_geojson = gdf_to_geojson(sediment_with_predictions)\n",
    "            \n",
    "            # Debug: Check the size of the predictions GeoJSON payload\n",
    "            flood_geojson_size_mb = len(flood_geojson) / (1024 * 1024)\n",
    "            print(f\"Predictions GeoJSON payload size: {flood_geojson_size_mb:.2f} MB\")\n",
    "            \n",
    "            # Debug: Check if GeoJSON has the expected prediction fields\n",
    "            flood_sample = json.loads(flood_geojson)[\"features\"][0] if \"features\" in json.loads(flood_geojson) else None\n",
    "            if flood_sample:\n",
    "                print(f\"Prediction fields: {[k for k in flood_sample['properties'].keys() if 'prediction' in k]}\")\n",
    "            \n",
    "            flood_source = GeoJSONDataSource(geojson=flood_geojson)\n",
    "            print(\"Create Flood Layer\")\n",
    "            # Create color mapper with initial field name for year 0\n",
    "            color_mapper = linear_cmap(\n",
    "                field_name='predictions_0',\n",
    "                palette=Viridis256,\n",
    "                low=0,\n",
    "                high=100  # Predictions are percentages (0-100)\n",
    "            )\n",
    "            \n",
    "            # Add flood risk layer\n",
    "            flood_layer = p.patches(\n",
    "                'xs', 'ys', \n",
    "                source=flood_source,\n",
    "                fill_color=color_mapper,\n",
    "                fill_alpha=0.7,\n",
    "                line_color=None,\n",
    "                legend_label=\"Flood Risk\"\n",
    "            )\n",
    "            \n",
    "            # Configure color bar\n",
    "            color_bar = ColorBar(\n",
    "                color_mapper=color_mapper['transform'],\n",
    "                location=(0, 0),\n",
    "                title=\"Flood Risk (%) - Year 0\",\n",
    "                ticker=bokeh.models.BasicTicker(desired_num_ticks=5),\n",
    "                formatter=bokeh.models.PrintfTickFormatter(format=\"%d%%\")\n",
    "            )\n",
    "            p.add_layout(color_bar, 'right')\n",
    "            \n",
    "            # Fixed slider callback with error handling to properly update the visualization\n",
    "            year_callback = CustomJS(\n",
    "                args=dict(\n",
    "                    flood_layer=flood_layer,\n",
    "                    flood_source=flood_source,\n",
    "                    slider=year_slider,\n",
    "                    color_bar=color_bar,\n",
    "                    mapper=color_mapper\n",
    "                ),\n",
    "                code=\"\"\"\n",
    "                    try {\n",
    "                        // Ensure we're in a browser context with a DOM\n",
    "                        if (typeof document === 'undefined' || document.body === null) {\n",
    "                            console.warn('DOM not ready yet, skipping callback');\n",
    "                            return;\n",
    "                        }\n",
    "                        \n",
    "                        // Get current year from slider\n",
    "                        const year = Math.round(slider.value);\n",
    "                        console.log('Changing visualization to year:', year);\n",
    "                        \n",
    "                        // Create the field name for this year's predictions\n",
    "                        const field_name = 'predictions_' + year;\n",
    "                        \n",
    "                        // Need to update the mapper's field name\n",
    "                        mapper.field = field_name;\n",
    "                        \n",
    "                        // Update the layer's glyph\n",
    "                        flood_layer.glyph.fill_color = {\n",
    "                            ...flood_layer.glyph.fill_color,\n",
    "                            field: field_name\n",
    "                        };\n",
    "                        \n",
    "                        // Update the color bar title\n",
    "                        color_bar.title = 'Flood Risk (%) - Year ' + year;\n",
    "                        \n",
    "                        // Force a data source change to trigger redraw\n",
    "                        flood_source.change.emit();\n",
    "                    } catch(e) {\n",
    "                        console.error('Error in year slider callback:', e);\n",
    "                    }\n",
    "                \"\"\"\n",
    "            )\n",
    "            year_slider.js_on_change('value', year_callback)\n",
    "            \n",
    "            # Create a single hover tool that will be dynamically updated\n",
    "            hover = HoverTool(\n",
    "                tooltips=[\n",
    "                    (\"Soil Type\", \"@sediment\"),\n",
    "                    (\"Elevation\", \"@elevation{0,0.0}\"),\n",
    "                    (\"Current Prediction (Year 0)\", \"@predictions_0{0.0}%\")  # This should be updated by the slider\n",
    "                ],\n",
    "                renderers=[flood_layer]  # Explicitly attach to flood layer\n",
    "            )\n",
    "            \n",
    "            # Improved tooltip callback with error handling\n",
    "            hover_callback = CustomJS(\n",
    "                args=dict(hover=hover, slider=year_slider),\n",
    "                code=\"\"\"\n",
    "                    try {\n",
    "                        // Get current year from slider\n",
    "                        const year = Math.round(slider.value);\n",
    "                        const field = 'predictions_' + year;\n",
    "                        \n",
    "                        // Update the hover tooltip with the current year\n",
    "                        hover.tooltips[2][0] = \"Current Prediction (Year \" + year + \")\";\n",
    "                        hover.tooltips[2][1] = \"@\" + field + \"{0.0}%\";\n",
    "                        \n",
    "                        // Force the hover tool to update\n",
    "                        hover.change.emit();\n",
    "                        console.log(\"Updated hover tooltip for year: \" + year);\n",
    "                    } catch(e) {\n",
    "                        console.error('Error in hover callback:', e);\n",
    "                    }\n",
    "                \"\"\"\n",
    "            )\n",
    "            \n",
    "            # Add the hover callback to the slider's change event\n",
    "            year_slider.js_on_change('value', hover_callback)\n",
    "            \n",
    "            # Add the hover tool to the plot\n",
    "            p.add_tools(hover)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR setting up flood predictions: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Layer visibility controls\n",
    "    layer_names = []\n",
    "    active_layers = []\n",
    "    \n",
    "    if sediment_layer:\n",
    "        layer_names.append(\"Sediment\")\n",
    "        active_layers.append(len(layer_names) - 1)\n",
    "    \n",
    "    if precipitation_layer:\n",
    "        layer_names.append(\"Precipitation Coverage\")\n",
    "        active_layers.append(len(layer_names) - 1)\n",
    "    \n",
    "    if flood_layer:\n",
    "        layer_names.append(\"Flood Risk\")\n",
    "        active_layers.append(len(layer_names) - 1)\n",
    "    \n",
    "    checkbox = CheckboxGroup(labels=layer_names, active=active_layers)\n",
    "    print(f\"Initial active layers: {active_layers}\")\n",
    "    \n",
    "    # JavaScript callback for layer visibility with error handling\n",
    "    js_args = {'checkbox': checkbox}\n",
    "    \n",
    "    if sediment_layer:\n",
    "        js_args['sediment_layer'] = sediment_layer\n",
    "    \n",
    "    if precipitation_layer:\n",
    "        js_args['precipitation_layer'] = precipitation_layer\n",
    "    \n",
    "    if station_layer:\n",
    "        js_args['station_layer'] = station_layer\n",
    "    \n",
    "    if flood_layer:\n",
    "        js_args['flood_layer'] = flood_layer\n",
    "    \n",
    "    checkbox_code = \"\"\"\n",
    "        try {\n",
    "            let i = 0;\n",
    "    \"\"\"\n",
    "    \n",
    "    if sediment_layer:\n",
    "        checkbox_code += \"\"\"\n",
    "            sediment_layer.visible = checkbox.active.includes(i);\n",
    "            i++;\n",
    "        \"\"\"\n",
    "    \n",
    "    if precipitation_layer:\n",
    "        checkbox_code += \"\"\"\n",
    "            precipitation_layer.visible = checkbox.active.includes(i);\n",
    "            if (typeof station_layer !== 'undefined')\n",
    "                station_layer.visible = checkbox.active.includes(i);\n",
    "            i++;\n",
    "        \"\"\"\n",
    "    \n",
    "    if flood_layer:\n",
    "        checkbox_code += \"\"\"\n",
    "            if (typeof flood_layer !== 'undefined')\n",
    "                flood_layer.visible = checkbox.active.includes(i);\n",
    "        \"\"\"\n",
    "    \n",
    "    checkbox_code += \"\"\"\n",
    "        } catch(e) {\n",
    "            console.error('Error in checkbox callback:', e);\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    checkbox_callback = CustomJS(args=js_args, code=checkbox_code)\n",
    "    checkbox.js_on_change('active', checkbox_callback)\n",
    "    \n",
    "    # Assemble layout\n",
    "    controls = column(year_slider, checkbox)\n",
    "        \n",
    "    layout = column(p, controls)\n",
    "    p.legend.location = \"top_left\"\n",
    "    p.legend.click_policy = \"hide\"\n",
    "    p.legend.title = \"Layers\"\n",
    "    \n",
    "    print(\"From the River to the Sea, Palestine will be Free!\")\n",
    "    return layout\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    p = init_map()\n",
    "    # Save to an HTML file and display in browser\n",
    "    output_file(\"terrain_map.html\")\n",
    "    show(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
